"""
å€å¡Šå®¢æ­·å²æ–°èçˆ¬èŸ² - å¾èˆŠåˆ°æ–°
å¾ç¬¬ 1235 é é–‹å§‹å¾€å‰çˆ¬å– (2018-2019 å¹´çš„æ–‡ç« )
"""
import asyncio
import json
import os
from datetime import datetime
from playwright.async_api import async_playwright
import re

class BlockcastHistoryScraper:
    def __init__(self):
        self.output_dir = 'output/news_history'
        os.makedirs(self.output_dir, exist_ok=True)
        # è¦éæ¿¾çš„é‡è¤‡æ–‡ç« ï¼ˆæ¯é éƒ½æœƒå‡ºç¾çš„ç½®é ‚æ–‡ç« ï¼‰
        self.skip_urls = [
            'https://blockcast.it/2025/11/14/ethereum-interoperability-the-final-mile-to-mass-adoption/'
        ]
        # æ‰¹æ¬¡å„²å­˜è¨­å®š
        self.batch_size = 30  # æ¯ 30 ç¯‡å¯«å…¥ä¸€æ¬¡
        self.current_batch = []  # ç•¶å‰æ‰¹æ¬¡çš„æ–‡ç« 
        self.total_saved = 0  # å·²å„²å­˜çš„æ–‡ç« ç¸½æ•¸
    
    def _save_batch(self, force=False):
        """
        å„²å­˜ç•¶å‰æ‰¹æ¬¡çš„æ–‡ç« 
        
        Args:
            force: æ˜¯å¦å¼·åˆ¶å„²å­˜ï¼ˆå³ä½¿æœªé”åˆ°æ‰¹æ¬¡å¤§å°ï¼‰
        """
        if not self.current_batch:
            return
        
        if not force and len(self.current_batch) < self.batch_size:
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # æŒ‰å¹´ä»½åˆ†é¡
        by_year = {}
        for article in self.current_batch:
            year = article.get('year')
            if year:
                if year not in by_year:
                    by_year[year] = []
                by_year[year].append(article)
        
        # å„²å­˜å„å¹´ä»½
        for year in sorted(by_year.keys()):
            articles = by_year[year]
            year_dir = os.path.join(self.output_dir, str(year))
            os.makedirs(year_dir, exist_ok=True)
            
            # ä½¿ç”¨ append æ¨¡å¼ç´¯åŠ æ–‡ç« 
            filename = os.path.join(year_dir, f'blockcast_batch_{timestamp}.json')
            
            output_data = {
                'year': year,
                'batch_articles': len(articles),
                'articles': articles,
                'saved_at': datetime.now().isoformat()
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ [{year}] å·²å„²å­˜ {len(articles)} ç¯‡ â†’ {filename}")
        
        self.total_saved += len(self.current_batch)
        print(f"âœ… ç´¯è¨ˆå·²å„²å­˜: {self.total_saved} ç¯‡æ–‡ç« \n")
        
        # æ¸…ç©ºç•¶å‰æ‰¹æ¬¡
        self.current_batch = []
    
    async def _fetch_article_content(self, page, article_url):
        """
        é€²å…¥æ–‡ç« è©³ç´°é é¢ï¼ŒæŠ“å–å®Œæ•´å…§æ–‡å’Œå¹´ä»½
        
        Args:
            page: Playwright page ç‰©ä»¶
            article_url: æ–‡ç« ç¶²å€
            
        Returns:
            dict: åŒ…å«å®Œæ•´å…§æ–‡ã€ç™¼å¸ƒæ—¥æœŸç­‰è³‡è¨Š
        """
        try:
            print(f"    ğŸ”— é€²å…¥æ–‡ç« : {article_url[:80]}")
            await page.goto(article_url, wait_until='networkidle', timeout=30000)
            await page.wait_for_timeout(1500)
            
            # æŠ“å–æ–‡ç« å…§æ–‡ - å˜—è©¦å¤šç¨®é¸æ“‡å™¨
            content = ''
            content_selectors = [
                '.entry-content',
                'article .entry-content',
                '.post-content',
                '.article-content',
                '.content',
                'main article'
            ]
            
            for selector in content_selectors:
                try:
                    content_elem = await page.query_selector(selector)
                    if content_elem:
                        content = await content_elem.inner_text()
                        if content and len(content) > 100:  # ç¢ºä¿æŠ“åˆ°å¯¦è³ªå…§å®¹
                            print(f"    âœ“ ä½¿ç”¨é¸æ“‡å™¨ '{selector}' æŠ“åˆ°å…§æ–‡")
                            break
                except:
                    continue
            
            # å¦‚æœæ²’æ‰¾åˆ°å®Œæ•´å…§å®¹ï¼Œå˜—è©¦æŠ“å–æ‰€æœ‰æ®µè½
            if not content or len(content) < 100:
                print(f"    âš ï¸  å˜—è©¦ç”¨æ®µè½æ–¹å¼æŠ“å–...")
                paragraphs = await page.query_selector_all('article p, .entry-content p, .post-content p')
                content_parts = []
                for p in paragraphs:
                    try:
                        text = await p.inner_text()
                        if text and len(text) > 20:  # éæ¿¾æ‰å¤ªçŸ­çš„æ®µè½
                            content_parts.append(text.strip())
                    except:
                        continue
                content = '\n\n'.join(content_parts)
            
            # å†æ¬¡å¾ URL ç¢ºèªå¹´ä»½ï¼ˆæœ€å¯é ï¼‰
            year = None
            date = ''
            url_match = re.search(r'/(\d{4})/(\d{2})/(\d{2})/', article_url)
            if url_match:
                year = int(url_match.group(1))
                month = url_match.group(2)
                day = url_match.group(3)
                date = f"{year}-{month}-{day}"
            
            return {
                'content': content.strip(),
                'year': year,
                'date': date
            }
            
        except Exception as e:
            print(f"    âŒ æŠ“å–æ–‡ç« å…§å®¹å¤±æ•—: {e}")
            return {
                'content': '',
                'year': None,
                'date': ''
            }
    
    async def scrape_blockcast_pages(self, page, start_page=1235, num_pages=50):
        """
        å¾æŒ‡å®šé é¢é–‹å§‹çˆ¬å–å€å¡Šå®¢æ–‡ç« 
        
        Args:
            page: Playwright page ç‰©ä»¶
            start_page: èµ·å§‹é ç¢¼ (é è¨­ 1, æœ€æ–°æ–‡ç« )
            num_pages: è¦çˆ¬å–çš„é æ•¸ (é è¨­ 1235, æ¶µè“‹æ‰€æœ‰æ­·å²)
        """
        print("\nğŸ” æ­£åœ¨çˆ¬å–ï¼šå€å¡Šå®¢æ­·å²æ–‡ç« ")
        print("=" * 60)
        print(f"èµ·å§‹é : ç¬¬ {start_page} é ")
        print(f"ç›®æ¨™: çˆ¬å– {num_pages} é ")
        print(f"ç¯„åœ: é ç¢¼ {start_page} â†’ {start_page + num_pages - 1}")
        print("=" * 60)
        
        all_articles = []
        
        for page_num in range(start_page, start_page + num_pages):  # å¾é ç¢¼ 1 é–‹å§‹å¾€å¾Œ
            if page_num > 1235:  # æœ€å¤šåˆ° 1235 é 
                break
            
            try:
                url = f"https://blockcast.it/category/news/page/{page_num}/"
                
                print(f"\nğŸ“„ ç¬¬ {page_num} é : {url}")
                
                await page.goto(url, wait_until='domcontentloaded', timeout=60000)
                await page.wait_for_timeout(2000)
                
                # çˆ¬å–æ–‡ç« åˆ—è¡¨ - ä½¿ç”¨æ›´å»£æ³›çš„é¸æ“‡å™¨
                article_elements = await page.query_selector_all('article')
                
                if not article_elements:
                    print(f"âš ï¸  ç¬¬ {page_num} é æ²’æœ‰æ‰¾åˆ°æ–‡ç« ")
                    continue
                
                print(f"æ‰¾åˆ° {len(article_elements)} ç¯‡æ–‡ç« ")
                
                # ğŸ”¥ ä¿®æ­£: å…ˆæ”¶é›†æ‰€æœ‰æ–‡ç« çš„åŸºæœ¬è³‡è¨Šï¼ˆæ¨™é¡Œã€é€£çµï¼‰ï¼Œå†é€ä¸€é€²å…¥æ–‡ç« é é¢
                article_links = []
                
                for elem in article_elements:
                    try:
                        # æå–æ¨™é¡Œå’Œé€£çµ
                        title = ''
                        link = ''
                        
                        # å…ˆæ‰¾åˆ°ä¸»è¦é€£çµ
                        link_elem = await elem.query_selector('a[rel="bookmark"], h3 a, h2 a, a')
                        if link_elem:
                            link = await link_elem.get_attribute('href')
                            title = await link_elem.inner_text()
                        
                        # å¦‚æœæ²’æ‰¾åˆ°,å†è©¦å…¶ä»–æ–¹æ³•
                        if not title:
                            title_elem = await elem.query_selector('h1, h2, h3, h4, .title')
                            title = await title_elem.inner_text() if title_elem else ''
                        
                        if link and not link.startswith('http'):
                            link = f"https://blockcast.it{link}"
                        
                        # ğŸ”¥ éæ¿¾æ‰ç½®é ‚é‡è¤‡æ–‡ç« 
                        if link in self.skip_urls:
                            continue
                        
                        # æå–æ‘˜è¦
                        summary_elem = await elem.query_selector('.excerpt, .summary, p')
                        summary = await summary_elem.inner_text() if summary_elem else ''
                        
                        if title and link:
                            article_links.append({
                                'title': title.strip(),
                                'link': link,
                                'summary': summary.strip()[:200]
                            })
                    
                    except Exception as e:
                        print(f"  âš ï¸  æ”¶é›†æ–‡ç« è³‡è¨Šæ™‚å‡ºéŒ¯: {str(e)[:50]}")
                        continue
                
                print(f"æˆåŠŸæ”¶é›† {len(article_links)} ç¯‡æ–‡ç« é€£çµ")
                
                # ğŸ”¥ ç¾åœ¨é€ä¸€é€²å…¥æ–‡ç« é é¢æŠ“å–å®Œæ•´å…§å®¹
                page_articles = []
                for idx, article_info in enumerate(article_links, 1):
                    try:
                        print(f"\n  [{idx}/{len(article_links)}] {article_info['title'][:50]}...")
                        
                        # é€²å…¥æ–‡ç« è©³ç´°é é¢æŠ“å–å®Œæ•´å…§å®¹
                        article_details = await self._fetch_article_content(page, article_info['link'])
                        
                        # å¾ URL æå–å¹´ä»½
                        year = None
                        date = ''
                        url_match = re.search(r'/(\d{4})/(\d{2})/(\d{2})/', article_info['link'])
                        if url_match:
                            year = int(url_match.group(1))
                            month = url_match.group(2)
                            day = url_match.group(3)
                            date = f"{year}-{month}-{day}"
                        
                        # å„ªå…ˆä½¿ç”¨æ–‡ç« é é¢çš„å¹´ä»½ï¼ˆæ›´æº–ç¢ºï¼‰
                        final_year = article_details['year'] if article_details['year'] else year
                        final_date = article_details['date'] if article_details['date'] else date
                        
                        article_data = {
                            'title': article_info['title'],
                            'link': article_info['link'],
                            'summary': article_info['summary'],
                            'content': article_details['content'],  # å®Œæ•´å…§æ–‡
                            'date': final_date.strip(),
                            'year': final_year,
                            'source': 'Blockcast',
                            'page_num': page_num,
                            'scraped_at': datetime.now().isoformat()
                        }
                        
                        page_articles.append(article_data)
                        all_articles.append(article_data)
                        
                        # ğŸ”¥ ç«‹å³åŠ å…¥æ‰¹æ¬¡ï¼Œä¸¦æª¢æŸ¥æ˜¯å¦éœ€è¦å„²å­˜
                        self.current_batch.append(article_data)
                        
                        # é¡¯ç¤ºå¹´ä»½è³‡è¨Šå’Œå…§æ–‡é•·åº¦
                        year_info = f"[{final_year}]" if final_year else "[?]"
                        content_len = len(article_details['content'])
                        print(f"  âœ“ {year_info} æŠ“å–æˆåŠŸ (å…§æ–‡: {content_len} å­—å…ƒ)")
                        
                        # ğŸ”¥ é”åˆ°æ‰¹æ¬¡å¤§å°æ™‚è‡ªå‹•å„²å­˜
                        if len(self.current_batch) >= self.batch_size:
                            print(f"\n{'='*60}")
                            print(f"ğŸ“¦ å·²ç´¯ç© {len(self.current_batch)} ç¯‡æ–‡ç« ï¼Œé–‹å§‹æ‰¹æ¬¡å„²å­˜...")
                            print(f"{'='*60}")
                            self._save_batch()
                    
                    except Exception as e:
                        print(f"  âŒ éŒ¯èª¤: {str(e)[:100]}")
                        continue
                
                print(f"æœ¬é æˆåŠŸæŠ“å–: {len(page_articles)} ç¯‡")
                
                # æ¯çˆ¬ 5 é ä¼‘æ¯ä¸€ä¸‹
                if (start_page - page_num + 1) % 5 == 0:
                    print("â¸ï¸  ä¼‘æ¯ 3 ç§’...")
                    await page.wait_for_timeout(3000)
                else:
                    await page.wait_for_timeout(1000)
                
            except Exception as e:
                print(f"âŒ ç¬¬ {page_num} é çˆ¬å–å¤±æ•—: {e}")
                continue
        
        # ğŸ”¥ çˆ¬å–å®Œæˆå¾Œï¼Œå¼·åˆ¶å„²å­˜å‰©é¤˜çš„æ–‡ç« 
        if self.current_batch:
            print(f"\n{'='*60}")
            print(f"ğŸ“¦ çˆ¬å–å®Œæˆï¼å„²å­˜å‰©é¤˜ {len(self.current_batch)} ç¯‡æ–‡ç« ...")
            print(f"{'='*60}")
            self._save_batch(force=True)
        
        print(f"\nâœ… å…±æˆåŠŸæŠ“å–: {len(all_articles)} ç¯‡æ–‡ç« ")
        print(f"ğŸ’¾ ç¸½å…±å„²å­˜: {self.total_saved} ç¯‡æ–‡ç« ")
        return all_articles
    
    def save_by_year(self, all_articles):
        """æŒ‰å¹´ä»½å„²å­˜æ–‡ç« """
        if not all_articles:
            print("\nâš ï¸  æ²’æœ‰æ‰¾åˆ°æ–‡ç« ")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # æŒ‰å¹´ä»½åˆ†é¡
        by_year = {}
        no_year = []
        
        for article in all_articles:
            year = article.get('year')
            if year:
                if year not in by_year:
                    by_year[year] = []
                by_year[year].append(article)
            else:
                no_year.append(article)
        
        # å„²å­˜å„å¹´ä»½
        for year in sorted(by_year.keys()):
            articles = by_year[year]
            year_dir = os.path.join(self.output_dir, str(year))
            os.makedirs(year_dir, exist_ok=True)
            
            filename = os.path.join(year_dir, f'blockcast_{timestamp}.json')
            
            output_data = {
                'year': year,
                'total_articles': len(articles),
                'articles': articles,
                'scraped_at': datetime.now().isoformat()
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ {year} å¹´: å·²å„²å­˜ {len(articles)} ç¯‡æ–‡ç« ")
            print(f"   æª”æ¡ˆ: {filename}")
        
        # å„²å­˜å¹´ä»½æœªçŸ¥çš„æ–‡ç« 
        if no_year:
            unknown_dir = os.path.join(self.output_dir, 'unknown_year')
            os.makedirs(unknown_dir, exist_ok=True)
            filename = os.path.join(unknown_dir, f'blockcast_{timestamp}.json')
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump({
                    'year': None,
                    'total_articles': len(no_year),
                    'articles': no_year,
                    'scraped_at': datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ å¹´ä»½æœªçŸ¥: å·²å„²å­˜ {len(no_year)} ç¯‡æ–‡ç« ")
        
        # ç”Ÿæˆç¸½è¦½
        summary_file = os.path.join(self.output_dir, f'summary_{timestamp}.json')
        summary = {
            'total_articles': len(all_articles),
            'by_year': {str(year): len(articles) for year, articles in by_year.items()},
            'unknown_year': len(no_year),
            'year_range': {
                'earliest': min(by_year.keys()) if by_year else None,
                'latest': max(by_year.keys()) if by_year else None
            },
            'scraped_at': datetime.now().isoformat()
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print("\n" + "=" * 60)
        print("ğŸ“Š ç¸½è¦½çµ±è¨ˆ")
        print("=" * 60)
        print(f"ç¸½å…±: {len(all_articles)} ç¯‡æ–‡ç« ")
        if summary['year_range']['earliest']:
            print(f"å¹´ä»½ç¯„åœ: {summary['year_range']['earliest']} - {summary['year_range']['latest']}")
        print(f"å„å¹´ä»½æ–‡ç« æ•¸:")
        for year in sorted(by_year.keys()):
            print(f"  {year}: {len(by_year[year])} ç¯‡")
        print(f"\nç¸½è¦½æª”æ¡ˆ: {summary_file}")
    
    async def scrape(self, start_page=1235, num_pages=50):
        """åŸ·è¡Œçˆ¬å–"""
        print("=" * 60)
        print("ğŸš€ é–‹å§‹çˆ¬å–å€å¡Šå®¢æ­·å²æ–°è")
        print("=" * 60)
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            all_articles = []
            
            try:
                articles = await self.scrape_blockcast_pages(page, start_page, num_pages)
                all_articles.extend(articles)
                
            except Exception as e:
                print(f"\nâŒ çˆ¬å–éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            
            finally:
                await browser.close()
            
            # ğŸ”¥ ä¸éœ€è¦å†æ¬¡å„²å­˜ï¼Œå› ç‚ºå·²ç¶“å‹•æ…‹å„²å­˜éäº†
            # åªç”Ÿæˆçµ±è¨ˆæ‘˜è¦
            if all_articles:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                summary_file = os.path.join(self.output_dir, f'summary_{timestamp}.json')
                
                by_year = {}
                for article in all_articles:
                    year = article.get('year')
                    if year:
                        if year not in by_year:
                            by_year[year] = []
                        by_year[year].append(article)
                
                summary = {
                    'total_articles': len(all_articles),
                    'by_year': {year: len(articles) for year, articles in by_year.items()},
                    'year_range': {
                        'earliest': min(by_year.keys()) if by_year else None,
                        'latest': max(by_year.keys()) if by_year else None
                    },
                    'total_saved': self.total_saved,
                    'scraped_at': datetime.now().isoformat()
                }
                
                with open(summary_file, 'w', encoding='utf-8') as f:
                    json.dump(summary, f, ensure_ascii=False, indent=2)
                
                print("\n" + "=" * 60)
                print("ğŸ“Š ç¸½è¦½çµ±è¨ˆ")
                print("=" * 60)
                print(f"ç¸½å…±æŠ“å–: {len(all_articles)} ç¯‡æ–‡ç« ")
                print(f"ç¸½å…±å„²å­˜: {self.total_saved} ç¯‡æ–‡ç« ")
                if summary['year_range']['earliest']:
                    print(f"å¹´ä»½ç¯„åœ: {summary['year_range']['earliest']} - {summary['year_range']['latest']}")
                print(f"å„å¹´ä»½æ–‡ç« æ•¸:")
                for year in sorted(by_year.keys()):
                    print(f"  {year}: {len(by_year[year])} ç¯‡")
                print(f"\nç¸½è¦½æª”æ¡ˆ: {summary_file}")
            
            print("\n" + "=" * 60)
            print("âœ… çˆ¬å–å®Œæˆ!")
            print("=" * 60)


async def main():
    print("ğŸ—ï¸  å€å¡Šå®¢æ­·å²æ–°èçˆ¬èŸ²")
    print("ğŸ“… æ¶µè“‹ç¯„åœ: 2017~2025 å¹´çš„æ‰€æœ‰æ–‡ç« ")
    print("ğŸ“„ é ç¢¼èªªæ˜: é ç¢¼ 1 = æœ€æ–° (2025), é ç¢¼ 1235 = æœ€èˆŠ (2017)")
    print()
    
    # è¼¸å…¥èµ·å§‹é ç¢¼
    try:
        start_page = int(input("å¾ç¬¬å¹¾é é–‹å§‹? (é è¨­ 1, æœ€æ–°æ–‡ç« ): ") or "1")
    except ValueError:
        start_page = 1
    
    # è¼¸å…¥è¦çˆ¬å–çš„é æ•¸
    try:
        num_pages = int(input("è¦çˆ¬å–å¹¾é ? (é è¨­ 1235, å…¨éƒ¨æ­·å²): ") or "1235")
    except ValueError:
        num_pages = 1235
    
    print()
    print(f"ğŸ¯ å°‡çˆ¬å–é ç¢¼ {start_page} â†’ {start_page + num_pages - 1}")
    print(f"ğŸ“Š é è¨ˆæ–‡ç« æ•¸: ~{num_pages * 11:,} ç¯‡")
    print()
    
    scraper = BlockcastHistoryScraper()
    await scraper.scrape(start_page=start_page, num_pages=num_pages)


if __name__ == "__main__":
    asyncio.run(main())
