"""
å¤šç¶²ç«™æ­·å²æ–°èçˆ¬èŸ²
æ”¯æ´: BlockTempo, ABMedia (å¸‚å ´/æ¯”ç‰¹å¹£)
å¾æœ€å¾Œä¸€é å¾€å‰çˆ¬å–åˆ°ç¬¬ 1 é ï¼Œæ”¶é›†å®Œæ•´å…§æ–‡
"""
import asyncio
import json
import os
from datetime import datetime
from playwright.async_api import async_playwright
import re

class MultiSiteHistoryScraper:
    def __init__(self):
        self.output_dir = 'output/news_history_multi'
        os.makedirs(self.output_dir, exist_ok=True)
        
        # æ‰¹æ¬¡å„²å­˜è¨­å®š
        self.batch_size = 30  # æ¯ 30 ç¯‡å¯«å…¥ä¸€æ¬¡
        self.current_batch = []  # ç•¶å‰æ‰¹æ¬¡çš„æ–‡ç« 
        self.total_saved = 0  # å·²å„²å­˜çš„æ–‡ç« ç¸½æ•¸
        
        # æ™‚é–“è¿½è¹¤ - ç”¨æ–¼æª¢æ¸¬å»£å‘Šæ–‡ç« 
        self.last_article_date = None  # ä¸Šä¸€ç¯‡æ–‡ç« çš„æ—¥æœŸ
        
        # ç¶²ç«™è¨­å®š
        self.sites_config = {
            'blockcast': {
                'name': 'Blockcast',
                'base_url': 'https://blockcast.it/category/news/page/',
                'start_page': 1235,  # æœ€å¾Œä¸€é 
                'end_page': 1,  # çˆ¬åˆ°ç¬¬ 1 é 
                'direction': 'backward',  # å¾€å‰çˆ¬ï¼ˆ1235â†’1ï¼‰
                'article_selector': 'article',
                'title_selector': 'a[rel="bookmark"], h3 a, h2 a, a',
                'link_selector': 'a[rel="bookmark"], h3 a, h2 a, a',
                'content_selector': '.entry-content',
                'skip_urls': [
                    'https://blockcast.it/2025/11/14/ethereum-interoperability-the-final-mile-to-mass-adoption/'
                ]
            },
            'blocktempo': {
                'name': 'BlockTempo',
                'base_url': 'https://www.blocktempo.com/category/cryptocurrency-market/exchange/page/',
                'start_page': 163,  # æœ€å¾Œä¸€é 
                'end_page': 1,
                'article_selector': 'article, .post, .article-item',
                'title_selector': 'h2 a, h3 a, .entry-title a',
                'link_selector': 'h2 a, h3 a, .entry-title a',
                'content_selector': '.entry-content, .post-content, article .content'
            },
            'btctech': {
                'name': 'BlockTempo-BTCtech',
                'base_url': 'https://www.blocktempo.com/category/technology/technology-bitcoin/page/',
                'start_page': 109,  # æœ€å¾Œä¸€é 
                'end_page': 1,
                'article_selector': 'article, .post, .article-item',
                'title_selector': 'h2 a, h3 a, .entry-title a',
                'link_selector': 'h2 a, h3 a, .entry-title a',
                'content_selector': '.entry-content, .post-content, article .content'
            },
            'marketanalytics': {
                'name': 'BlockTempo-MarketAnalytics',
                'base_url': 'https://www.blocktempo.com/category/cryptocurrency-market/market-analyze/page/',
                'start_page': 143,  # æœ€å¾Œä¸€é 
                'end_page': 1,
                'article_selector': 'article, .post, .article-item',
                'title_selector': 'h2 a, h3 a, .entry-title a',
                'link_selector': 'h2 a, h3 a, .entry-title a',
                'content_selector': '.entry-content, .post-content, article .content'
            },
            'abmedia_market': {
                'name': 'ABMedia-å¸‚å ´',
                'base_url': 'https://abmedia.io/category/invsetments/market/page/',
                'start_page': 196,  # æœ€å¾Œä¸€é 
                'end_page': 1,
                'article_selector': 'article, .post, .article-item',
                'title_selector': 'h2 a, h3 a, .entry-title a',
                'link_selector': 'h2 a, h3 a, .entry-title a',
                'content_selector': '.entry-content, .post-content, article .content'
            },
            'abmedia_bitcoin': {
                'name': 'ABMedia-æ¯”ç‰¹å¹£',
                'base_url': 'https://abmedia.io/category/invsetments/bitcoin/page/',
                'start_page': 119,  # æœ€å¾Œä¸€é 
                'end_page': 1,
                'article_selector': 'article, .post, .article-item',
                'title_selector': 'h2 a, h3 a, .entry-title a',
                'link_selector': 'h2 a, h3 a, .entry-title a',
                'content_selector': '.entry-content, .post-content, article .content'
            }
        }
    
    def _save_batch(self, site_name, force=False):
        """
        å„²å­˜ç•¶å‰æ‰¹æ¬¡çš„æ–‡ç« 
        
        Args:
            site_name: ç¶²ç«™åç¨±
            force: æ˜¯å¦å¼·åˆ¶å„²å­˜ï¼ˆå³ä½¿æœªé”åˆ°æ‰¹æ¬¡å¤§å°ï¼‰
        """
        if not self.current_batch:
            return
        
        if not force and len(self.current_batch) < self.batch_size:
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # æŒ‰å¹´ä»½åˆ†é¡
        by_year = {}
        for article in self.current_batch:
            year = article.get('year')
            if not year:
                year = 'unknown'  # ğŸ”¥ ä¿®å¾©ï¼šæ²’æœ‰å¹´ä»½çš„æ–‡ç« å­˜åˆ° unknown è³‡æ–™å¤¾
            if year not in by_year:
                by_year[year] = []
            by_year[year].append(article)
        
        # å„²å­˜å„å¹´ä»½
        for year in sorted(by_year.keys(), key=lambda x: (x == 'unknown', x)):
            articles = by_year[year]
            year_dir = os.path.join(self.output_dir, site_name, str(year))
            os.makedirs(year_dir, exist_ok=True)
            
            filename = os.path.join(year_dir, f'{site_name}_batch_{timestamp}.json')
            
            output_data = {
                'site': site_name,
                'year': year,
                'batch_articles': len(articles),
                'articles': articles,
                'saved_at': datetime.now().isoformat()
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ [{site_name}] [{year}] å·²å„²å­˜ {len(articles)} ç¯‡ â†’ {filename}")
        
        self.total_saved += len(self.current_batch)
        print(f"âœ… [{site_name}] ç´¯è¨ˆå·²å„²å­˜: {self.total_saved} ç¯‡æ–‡ç« \n")
        
        # æ¸…ç©ºç•¶å‰æ‰¹æ¬¡
        self.current_batch = []
    
    def _is_advertisement(self, article_data, site_name, direction='backward'):
        """
        åˆ¤æ–·æ–‡ç« æ˜¯å¦ç‚ºå»£å‘Š
        ä½¿ç”¨æ™‚é–“é€£çºŒæ€§åˆ¤æ–·ï¼šæ­£å¸¸æ–‡ç« æ—¥æœŸæ‡‰è©²é€£çºŒï¼Œå»£å‘Šæœƒçªç„¶è·³åˆ°æœ€æ–°æ—¥æœŸ
        
        Args:
            article_data: æ–‡ç« è³‡æ–™
            site_name: ç¶²ç«™åç¨±
            direction: çˆ¬å–æ–¹å‘ ('forward' æˆ– 'backward')
            
        Returns:
            bool: True = å»£å‘Š, False = æ­£å¸¸æ–‡ç« 
        """
        title = article_data.get('title', '')
        link = article_data.get('link', '')
        content = article_data.get('content', '')
        date_str = article_data.get('date', '')
        
        # 1. æ¨™é¡Œéæ¿¾ - Podcast ç¯€ç›®
        if title.startswith('EP.'):
            print(f"    âš ï¸  è·³é Podcast: {title[:50]}")
            return True
        
        # 2. URL éæ¿¾ - å»£å‘Šé€£çµ
        if 'news-list?source=' in link or '/ep-' in link.lower():
            print(f"    âš ï¸  è·³éå»£å‘Šé€£çµ: {link[:60]}")
            return True
        
        # 3. å…§æ–‡é•·åº¦éæ¿¾
        if len(content) < 300:
            print(f"    âš ï¸  è·³éçŸ­æ–‡ç«  ({len(content)} å­—å…ƒ): {title[:40]}")
            return True
        
        # 4. å›ºå®šæ–‡å­—éæ¿¾ - Podcast å›ºå®šé–‹å ´ç™½
        if 'ä¿è­‰å­¸ä¸åˆ°æ±è¥¿çš„ä¸è² è²¬ä»»å€å¡Šéˆæ™‚äº‹é›œè«‡' in content:
            print(f"    âš ï¸  è·³é Podcast å…§å®¹: {title[:40]}")
            return True
        
        # 5. æ™‚é–“é€£çºŒæ€§æª¢æŸ¥ï¼ˆåƒ…åœ¨ forward æ¨¡å¼å•Ÿç”¨ï¼‰
        # backward æ¨¡å¼ï¼ˆå¾èˆŠåˆ°æ–°ï¼‰æœƒè‡ªç„¶ç”¢ç”Ÿæ™‚é–“è·³èºï¼Œä¸æ‡‰åˆ¤å®šç‚ºå»£å‘Š
        if direction == 'forward' and date_str and self.last_article_date:
            try:
                from datetime import datetime
                
                # è§£ææ—¥æœŸæ ¼å¼ (æ”¯æ´ 2025/11/10 æˆ– 2025-11-10)
                current_date = None
                if '/' in date_str:
                    current_date = datetime.strptime(date_str, '%Y/%m/%d')
                elif '-' in date_str:
                    current_date = datetime.strptime(date_str.split()[0], '%Y-%m-%d')
                
                if current_date and self.last_article_date:
                    # è¨ˆç®—æ™‚é–“å·®ï¼ˆå¤©æ•¸ï¼‰
                    time_diff = abs((current_date - self.last_article_date).days)
                    
                    # å¦‚æœæ™‚é–“å·®è¶…é 180 å¤©ï¼ˆç´„ 6 å€‹æœˆï¼‰ï¼Œåˆ¤å®šç‚ºå»£å‘Š
                    if time_diff > 180:
                        print(f"    âš ï¸  æ™‚é–“è·³èºéå¤§ ({time_diff} å¤©): {date_str} â†’ {self.last_article_date.strftime('%Y/%m/%d')}")
                        print(f"       è·³éå¯èƒ½çš„å»£å‘Š: {title[:40]}")
                        return True
                
                # æ›´æ–°æœ€å¾Œæ–‡ç« æ—¥æœŸ
                if current_date:
                    self.last_article_date = current_date
            
            except Exception as e:
                pass  # æ—¥æœŸè§£æå¤±æ•—ï¼Œç¹¼çºŒè™•ç†
        else:
            # åˆå§‹åŒ–æœ€å¾Œæ–‡ç« æ—¥æœŸ
            if date_str:
                try:
                    from datetime import datetime
                    if '/' in date_str:
                        self.last_article_date = datetime.strptime(date_str, '%Y/%m/%d')
                    elif '-' in date_str:
                        self.last_article_date = datetime.strptime(date_str.split()[0], '%Y-%m-%d')
                except:
                    pass
        
        return False
    
    async def _fetch_article_content(self, page, article_url, site_name):
        """
        é€²å…¥æ–‡ç« è©³ç´°é é¢ï¼ŒæŠ“å–å®Œæ•´å…§æ–‡å’Œå¹´ä»½
        
        Args:
            page: Playwright page ç‰©ä»¶
            article_url: æ–‡ç« ç¶²å€
            site_name: ç¶²ç«™åç¨±
            
        Returns:
            dict: åŒ…å«å®Œæ•´å…§æ–‡ã€ç™¼å¸ƒæ—¥æœŸç­‰è³‡è¨Š
        """
        try:
            print(f"    ğŸ”— é€²å…¥æ–‡ç« : {article_url[:80]}")
            await page.goto(article_url, wait_until='networkidle', timeout=30000)
            await page.wait_for_timeout(1500)
            
            config = self.sites_config.get(site_name, {})
            
            # æŠ“å–æ–‡ç« å…§æ–‡
            content = ''
            content_selectors = [
                config.get('content_selector', '.entry-content'),
                '.entry-content',
                '.post-content',
                '.article-content',
                'article .content',
                'main article'
            ]
            
            for selector in content_selectors:
                try:
                    content_elem = await page.query_selector(selector)
                    if content_elem:
                        content = await content_elem.inner_text()
                        if content and len(content) > 100:
                            print(f"    âœ“ ä½¿ç”¨é¸æ“‡å™¨ '{selector}' æŠ“åˆ°å…§æ–‡")
                            break
                except:
                    continue
            
            # å¦‚æœæ²’æ‰¾åˆ°å®Œæ•´å…§å®¹ï¼Œå˜—è©¦æŠ“å–æ‰€æœ‰æ®µè½
            if not content or len(content) < 100:
                print(f"    âš ï¸  å˜—è©¦ç”¨æ®µè½æ–¹å¼æŠ“å–...")
                paragraphs = await page.query_selector_all('article p, .entry-content p, .post-content p')
                content_parts = []
                for p in paragraphs:
                    try:
                        text = await p.inner_text()
                        if text and len(text) > 20:
                            content_parts.append(text.strip())
                    except:
                        continue
                content = '\n\n'.join(content_parts)
            
            # å¾ URL æå–å¹´ä»½
            year = None
            date = ''
            
            # å˜—è©¦å¤šç¨®æ—¥æœŸæ ¼å¼
            url_patterns = [
                r'/(\d{4})/(\d{2})/(\d{2})/',  # /2023/11/15/
                r'/(\d{4})-(\d{2})-(\d{2})/',  # /2023-11-15/
                r'/(\d{4})(\d{2})(\d{2})/',    # /20231115/
            ]
            
            for pattern in url_patterns:
                url_match = re.search(pattern, article_url)
                if url_match:
                    year = int(url_match.group(1))
                    month = url_match.group(2)
                    day = url_match.group(3)
                    date = f"{year}-{month}-{day}"
                    break
            
            # å¦‚æœ URL æ²’æœ‰æ—¥æœŸï¼Œå˜—è©¦å¾é é¢æŠ“å–
            if not year:
                # ğŸ”¥ å„ªå…ˆå˜—è©¦ meta æ¨™ç±¤ (BlockTempo ç”¨é€™å€‹)
                meta_elem = await page.query_selector('meta[property="article:published_time"]')
                if meta_elem:
                    date_text = await meta_elem.get_attribute('content')
                    if date_text:
                        # è§£æ ISO 8601 æ ¼å¼: 2017-12-18T15:36:40+08:00
                        year_match = re.search(r'(\d{4})-(\d{2})-(\d{2})', date_text)
                        if year_match:
                            year = int(year_match.group(1))
                            date = f"{year_match.group(1)}/{year_match.group(2)}/{year_match.group(3)}"
                
                # å¦‚æœ meta æ²’æœ‰ï¼Œå†è©¦å…¶ä»–é¸æ“‡å™¨
                if not year:
                    date_elem = await page.query_selector('time, .post-date, .entry-date, [datetime]')
                    if date_elem:
                        date_text = await date_elem.inner_text() or await date_elem.get_attribute('datetime')
                        if date_text:
                            year_match = re.search(r'20\d{2}', date_text)
                            if year_match:
                                year = int(year_match.group())
                            date = date_text
            
            return {
                'content': content.strip(),
                'year': year,
                'date': date
            }
            
        except Exception as e:
            print(f"    âŒ æŠ“å–æ–‡ç« å…§å®¹å¤±æ•—: {e}")
            return {
                'content': '',
                'year': None,
                'date': ''
            }
    
    async def scrape_site(self, page, site_key, start_page=None, num_pages=None):
        """
        çˆ¬å–æŒ‡å®šç¶²ç«™çš„æ­·å²æ–‡ç« 
        
        Args:
            page: Playwright page ç‰©ä»¶
            site_key: ç¶²ç«™è­˜åˆ¥ç¢¼
            start_page: èµ·å§‹é ç¢¼
            num_pages: è¦çˆ¬å–çš„é æ•¸
        """
        config = self.sites_config[site_key]
        site_name = config['name']
        direction = config.get('direction', 'backward')  # é è¨­å¾€å‰çˆ¬
        skip_urls = config.get('skip_urls', [])
        
        # ğŸ”¥ é‡ç½®æ™‚é–“è¿½è¹¤ï¼ˆæ¯å€‹ç¶²ç«™ç¨ç«‹è¿½è¹¤ï¼‰
        self.last_article_date = None
        
        # ä½¿ç”¨è¨­å®šçš„èµ·å§‹é ç¢¼ï¼Œæˆ–å¾åƒæ•¸è¦†è“‹
        if start_page is None:
            start_page = config['start_page']
        
        # è¨ˆç®—çµæŸé ç¢¼å’Œé æ•¸ç¯„åœ
        if direction == 'forward':
            # å¾€å¾Œçˆ¬ï¼ˆ1â†’1235ï¼‰
            if num_pages:
                end_page = min(start_page + num_pages - 1, config['end_page'])
            else:
                end_page = config['end_page']
            page_range = range(start_page, end_page + 1)
            total_pages = end_page - start_page + 1
        else:
            # å¾€å‰çˆ¬ï¼ˆ163â†’1ï¼‰
            if num_pages:
                end_page = max(start_page - num_pages + 1, config['end_page'])
            else:
                end_page = config['end_page']
            page_range = range(start_page, end_page - 1, -1)
            total_pages = start_page - end_page + 1
        
        print(f"\nğŸ” æ­£åœ¨çˆ¬å–ï¼š{site_name}")
        print("=" * 60)
        print(f"èµ·å§‹é : ç¬¬ {start_page} é ")
        print(f"çµæŸé : ç¬¬ {end_page} é ")
        print(f"æ–¹å‘: {'å¾€å¾Œ' if direction == 'forward' else 'å¾€å‰'}")
        print(f"ç›®æ¨™: çˆ¬å– {total_pages} é ")
        print("=" * 60)
        
        all_articles = []
        
        # æ ¹æ“šæ–¹å‘çˆ¬å–
        for page_num in page_range:
            try:
                url = f"{config['base_url']}{page_num}/"
                
                print(f"\nğŸ“„ ç¬¬ {page_num} é : {url}")
                
                await page.goto(url, wait_until='domcontentloaded', timeout=60000)
                await page.wait_for_timeout(2000)
                
                # çˆ¬å–æ–‡ç« åˆ—è¡¨
                article_elements = await page.query_selector_all(config['article_selector'])
                
                if not article_elements:
                    print(f"âš ï¸  ç¬¬ {page_num} é æ²’æœ‰æ‰¾åˆ°æ–‡ç« ")
                    continue
                
                print(f"æ‰¾åˆ° {len(article_elements)} ç¯‡æ–‡ç« ")
                
                # å…ˆæ”¶é›†æ‰€æœ‰æ–‡ç« çš„åŸºæœ¬è³‡è¨Š
                article_links = []
                
                for elem in article_elements:
                    try:
                        # æå–æ¨™é¡Œå’Œé€£çµ
                        link_elem = await elem.query_selector(config['link_selector'])
                        if link_elem:
                            link = await link_elem.get_attribute('href')
                            title = await link_elem.inner_text()
                            
                            # å¦‚æœæ²’æ‰¾åˆ°æ¨™é¡Œï¼Œå†è©¦å…¶ä»–é¸æ“‡å™¨
                            if not title:
                                title_elem = await elem.query_selector('h1, h2, h3, h4, .title')
                                title = await title_elem.inner_text() if title_elem else ''
                            
                            if link and not link.startswith('http'):
                                # è£œå…¨ç¶²å€
                                if 'blocktempo' in site_key or 'btctech' in site_key or 'marketanalytics' in site_key:
                                    link = f"https://www.blocktempo.com{link}"
                                elif 'abmedia' in site_key:
                                    link = f"https://abmedia.io{link}"
                                elif 'blockcast' in site_key:
                                    link = f"https://blockcast.it{link}"
                            
                            # ğŸ”¥ æª¢æŸ¥æ˜¯å¦ç‚ºè¦è·³éçš„ URLï¼ˆç½®é ‚æ–‡ç« ï¼‰
                            if link in skip_urls:
                                print(f"    âš ï¸  è·³éç½®é ‚æ–‡ç« : {title[:40] if title else link[:60]}")
                                continue
                            
                            # æå–æ‘˜è¦
                            summary_elem = await elem.query_selector('.excerpt, .summary, p')
                            summary = await summary_elem.inner_text() if summary_elem else ''
                            
                            if title and link:
                                article_links.append({
                                    'title': title.strip(),
                                    'link': link,
                                    'summary': summary.strip()[:200]
                                })
                    
                    except Exception as e:
                        continue
                
                print(f"æˆåŠŸæ”¶é›† {len(article_links)} ç¯‡æ–‡ç« é€£çµ")
                
                # é€ä¸€é€²å…¥æ–‡ç« é é¢æŠ“å–å®Œæ•´å…§å®¹
                page_articles = []
                for idx, article_info in enumerate(article_links, 1):
                    try:
                        print(f"\n  [{idx}/{len(article_links)}] {article_info['title'][:50]}...")
                        
                        # é€²å…¥æ–‡ç« è©³ç´°é é¢
                        article_details = await self._fetch_article_content(page, article_info['link'], site_key)
                        
                        # å¾ URL æå–å¹´ä»½ï¼ˆå‚™ç”¨ï¼‰
                        year = None
                        date = ''
                        url_match = re.search(r'/(\d{4})/(\d{2})/(\d{2})/', article_info['link'])
                        if url_match:
                            year = int(url_match.group(1))
                            date = f"{url_match.group(1)}/{url_match.group(2)}/{url_match.group(3)}"
                        
                        # å„ªå…ˆä½¿ç”¨æ–‡ç« é é¢çš„å¹´ä»½
                        final_year = article_details['year'] if article_details['year'] else year
                        final_date = article_details['date'] if article_details['date'] else date
                        
                        article_data = {
                            'title': article_info['title'],
                            'link': article_info['link'],
                            'summary': article_info['summary'],
                            'content': article_details['content'],
                            'date': final_date,
                            'year': final_year,
                            'source': site_name,
                            'page_num': page_num,
                            'scraped_at': datetime.now().isoformat()
                        }
                        
                        # ğŸ”¥ æª¢æŸ¥æ˜¯å¦ç‚ºå»£å‘Šï¼ˆå‚³å…¥çˆ¬å–æ–¹å‘ï¼‰
                        if self._is_advertisement(article_data, site_name, direction):
                            continue
                        
                        page_articles.append(article_data)
                        all_articles.append(article_data)
                        
                        # åŠ å…¥æ‰¹æ¬¡
                        self.current_batch.append(article_data)
                        
                        # é¡¯ç¤ºé€²åº¦
                        year_info = f"[{final_year}]" if final_year else "[?]"
                        content_len = len(article_details['content'])
                        print(f"  âœ“ {year_info} æŠ“å–æˆåŠŸ (å…§æ–‡: {content_len} å­—å…ƒ)")
                        
                        # é”åˆ°æ‰¹æ¬¡å¤§å°æ™‚è‡ªå‹•å„²å­˜
                        if len(self.current_batch) >= self.batch_size:
                            print(f"\n{'='*60}")
                            print(f"ğŸ“¦ å·²ç´¯ç© {len(self.current_batch)} ç¯‡æ–‡ç« ï¼Œé–‹å§‹æ‰¹æ¬¡å„²å­˜...")
                            print(f"{'='*60}")
                            self._save_batch(site_name)
                    
                    except Exception as e:
                        print(f"  âŒ éŒ¯èª¤: {str(e)[:100]}")
                        continue
                
                print(f"æœ¬é æˆåŠŸæŠ“å–: {len(page_articles)} ç¯‡")
                
                # æ¯çˆ¬ 5 é ä¼‘æ¯ä¸€ä¸‹
                if (start_page - page_num + 1) % 5 == 0:
                    print("â¸ï¸  ä¼‘æ¯ 3 ç§’...")
                    await page.wait_for_timeout(3000)
                else:
                    await page.wait_for_timeout(1000)
                
            except Exception as e:
                print(f"âŒ ç¬¬ {page_num} é çˆ¬å–å¤±æ•—: {e}")
                continue
        
        # çˆ¬å–å®Œæˆå¾Œï¼Œå¼·åˆ¶å„²å­˜å‰©é¤˜çš„æ–‡ç« 
        if self.current_batch:
            print(f"\n{'='*60}")
            print(f"ğŸ“¦ [{site_name}] çˆ¬å–å®Œæˆï¼å„²å­˜å‰©é¤˜ {len(self.current_batch)} ç¯‡æ–‡ç« ...")
            print(f"{'='*60}")
            self._save_batch(site_name, force=True)
        
        print(f"\nâœ… [{site_name}] å…±æˆåŠŸæŠ“å–: {len(all_articles)} ç¯‡æ–‡ç« ")
        print(f"ğŸ’¾ [{site_name}] ç¸½å…±å„²å­˜: {self.total_saved} ç¯‡æ–‡ç« ")
        
        return all_articles
    
    async def scrape_all_sites(self, sites=None, num_pages=None):
        """
        çˆ¬å–æ‰€æœ‰ç¶²ç«™æˆ–æŒ‡å®šç¶²ç«™
        
        Args:
            sites: è¦çˆ¬å–çš„ç¶²ç«™åˆ—è¡¨ ['blocktempo', 'abmedia_market', 'abmedia_bitcoin']
                   å¦‚æœç‚º Noneï¼Œå‰‡çˆ¬å–æ‰€æœ‰ç¶²ç«™
            num_pages: æ¯å€‹ç¶²ç«™è¦çˆ¬å–çš„é æ•¸ï¼ˆå¾æœ€å¾Œä¸€é å¾€å‰ï¼‰
        """
        if sites is None:
            sites = list(self.sites_config.keys())
        
        print("=" * 60)
        print("ğŸš€ é–‹å§‹å¤šç¶²ç«™æ­·å²æ–°èçˆ¬å–")
        print("=" * 60)
        print(f"ç›®æ¨™ç¶²ç«™: {', '.join([self.sites_config[s]['name'] for s in sites])}")
        print("=" * 60)
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)  # ğŸ”¥ ç„¡é ­æ¨¡å¼ï¼Œä¸é¡¯ç¤ºè¦–çª—
            page = await browser.new_page()
            
            all_results = {}
            
            try:
                for site_key in sites:
                    print(f"\n\n{'='*60}")
                    print(f"é–‹å§‹çˆ¬å–: {self.sites_config[site_key]['name']}")
                    print(f"{'='*60}")
                    
                    # é‡ç½®æ‰¹æ¬¡è¨ˆæ•¸
                    self.current_batch = []
                    self.total_saved = 0
                    self.last_article_date = None  # ğŸ”¥ é‡ç½®æ™‚é–“è¿½è¹¤
                    
                    articles = await self.scrape_site(page, site_key, num_pages=num_pages)
                    all_results[site_key] = articles
                    
                    print(f"\nâœ… {self.sites_config[site_key]['name']} å®Œæˆï¼")
                    print(f"   å…±æŠ“å– {len(articles)} ç¯‡æ–‡ç« ")
                    
            except Exception as e:
                print(f"\nâŒ çˆ¬å–éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            
            finally:
                await browser.close()
            
            # ç”Ÿæˆç¸½è¦½å ±å‘Š
            print("\n" + "=" * 60)
            print("ğŸ“Š æ‰€æœ‰ç¶²ç«™çˆ¬å–å®Œæˆï¼")
            print("=" * 60)
            for site_key, articles in all_results.items():
                print(f"{self.sites_config[site_key]['name']}: {len(articles)} ç¯‡æ–‡ç« ")
            print("=" * 60)
            
            return all_results


async def main():
    print("ğŸ—ï¸  å¤šç¶²ç«™æ­·å²æ–°èçˆ¬èŸ²")
    print("=" * 60)
    print("æ”¯æ´ç¶²ç«™:")
    print("  1. Blockcast å€å¡Šå®¢ (1235 é )")
    print("  2. BlockTempo äº¤æ˜“æ‰€åˆ†é¡ (163 é )")
    print("  3. BlockTempo-BTCtech æ¯”ç‰¹å¹£æŠ€è¡“åˆ†é¡ (109 é )")
    print("  4. BlockTempo-MarketAnalytics å¸‚å ´åˆ†æåˆ†é¡ (143 é )")
    print("  5. ABMedia æŠ•è³‡å¸‚å ´ (196 é )")
    print("  6. ABMedia æ¯”ç‰¹å¹£ (119 é )")
    print("=" * 60)
    print()
    
    scraper = MultiSiteHistoryScraper()
    
    # ğŸ”¥ å®Œæ•´æ¨¡å¼ï¼šçˆ¬å–æ‰€æœ‰æ­·å²é é¢
    print("ğŸ”¥ å®Œæ•´æ¨¡å¼ï¼šçˆ¬å–æ‰€æœ‰æ­·å²é é¢ï¼ˆå…± 1,965 é ï¼‰")
    await scraper.scrape_all_sites()  # ä¸å‚³ num_pages åƒæ•¸ = çˆ¬å–å…¨éƒ¨


if __name__ == "__main__":
    asyncio.run(main())
