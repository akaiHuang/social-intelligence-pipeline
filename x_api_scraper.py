"""
X API çˆ¬èŸ²è…³æœ¬
ä½¿ç”¨å®˜æ–¹ X API v2 æŠ“å–æ¨æ–‡
"""
import os
import json
import requests
from datetime import datetime
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

class XAPIScraper:
    def __init__(self):
        # å¾ç’°å¢ƒè®Šæ•¸è®€å– API æ†‘è­‰
        self.bearer_token = os.getenv('X_BEARER_TOKEN')
        if not self.bearer_token:
            raise ValueError("è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š X_BEARER_TOKEN")
        
        self.base_url = "https://api.twitter.com/2"
        self.headers = {
            "Authorization": f"Bearer {self.bearer_token}",
            "User-Agent": "v2UserTweetsPython"
        }
    
    def get_user_id(self, username):
        """
        æ ¹æ“šä½¿ç”¨è€…åç¨±å–å¾— User ID
        
        Args:
            username: X ä½¿ç”¨è€…åç¨± (ä¸å« @)
        
        Returns:
            user_id: ä½¿ç”¨è€… ID
        """
        url = f"{self.base_url}/users/by/username/{username}"
        
        params = {
            "user.fields": "id,name,username,created_at,description,public_metrics,verified"
        }
        
        response = requests.get(url, headers=self.headers, params=params)
        
        if response.status_code != 200:
            raise Exception(
                f"ç„¡æ³•å–å¾—ä½¿ç”¨è€…è³‡è¨Š (HTTP {response.status_code}): {response.text}"
            )
        
        data = response.json()
        print(f"âœ“ æ‰¾åˆ°ä½¿ç”¨è€…: @{data['data']['username']}")
        print(f"  åç¨±: {data['data']['name']}")
        print(f"  ID: {data['data']['id']}")
        print(f"  è¿½è¹¤è€…: {data['data']['public_metrics']['followers_count']:,}")
        print()
        
        return data['data']['id'], data['data']
    
    def get_user_tweets(self, user_id, max_results=100, start_time=None):
        """
        å–å¾—ä½¿ç”¨è€…çš„æ¨æ–‡
        
        Args:
            user_id: ä½¿ç”¨è€… ID
            max_results: æ¯æ¬¡è«‹æ±‚çš„æœ€å¤§çµæœæ•¸ (5-100,å…è²»ç‰ˆå¯èƒ½æœ‰é™åˆ¶)
            start_time: é–‹å§‹æ™‚é–“ (ISO 8601 æ ¼å¼,ä¾‹å¦‚: 2020-01-01T00:00:00Z)
        
        Returns:
            tweets: æ¨æ–‡åˆ—è¡¨
        """
        url = f"{self.base_url}/users/{user_id}/tweets"
        
        # è¨­å®šè«‹æ±‚åƒæ•¸
        params = {
            "max_results": min(max_results, 100),  # API ä¸Šé™ 100
            "tweet.fields": "id,text,created_at,author_id,public_metrics,referenced_tweets,entities",
            "expansions": "referenced_tweets.id,author_id",
            "user.fields": "username,name,verified"
        }
        
        # å¦‚æœæŒ‡å®šé–‹å§‹æ™‚é–“ (éœ€è¦ Basic ä»¥ä¸Šæ–¹æ¡ˆ)
        if start_time:
            params["start_time"] = start_time
        
        all_tweets = []
        next_token = None
        page = 0
        
        print("ğŸ” é–‹å§‹æŠ“å–æ¨æ–‡...")
        
        while True:
            page += 1
            
            if next_token:
                params["pagination_token"] = next_token
            
            response = requests.get(url, headers=self.headers, params=params)
            
            if response.status_code != 200:
                print(f"\nâŒ API è«‹æ±‚å¤±æ•— (HTTP {response.status_code})")
                print(f"éŒ¯èª¤è¨Šæ¯: {response.text}")
                break
            
            data = response.json()
            
            # æª¢æŸ¥æ˜¯å¦æœ‰è³‡æ–™
            if "data" not in data or not data["data"]:
                print("\nâš ï¸  æ²’æœ‰æ›´å¤šæ¨æ–‡")
                break
            
            tweets = data["data"]
            all_tweets.extend(tweets)
            
            print(f"ğŸ“„ ç¬¬ {page} é : å–å¾— {len(tweets)} å‰‡æ¨æ–‡ (ç¸½è¨ˆ: {len(all_tweets)})")
            
            # é¡¯ç¤ºæœ€æ–°å’Œæœ€èˆŠçš„æ¨æ–‡æ™‚é–“
            if tweets:
                oldest = tweets[-1]['created_at']
                newest = tweets[0]['created_at']
                print(f"   æ™‚é–“ç¯„åœ: {oldest[:10]} ~ {newest[:10]}")
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é 
            if "meta" in data and "next_token" in data["meta"]:
                next_token = data["meta"]["next_token"]
            else:
                print("\nâœ“ å·²æŠ“å–æ‰€æœ‰å¯ç”¨æ¨æ–‡")
                break
        
        return all_tweets
    
    def save_tweets(self, tweets, user_data, filename=None):
        """
        å„²å­˜æ¨æ–‡åˆ° JSON æª”æ¡ˆ
        
        Args:
            tweets: æ¨æ–‡åˆ—è¡¨
            user_data: ä½¿ç”¨è€…è³‡æ–™
            filename: æª”æ¡ˆåç¨± (å¯é¸)
        """
        if not filename:
            username = user_data['username']
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            os.makedirs(f'output/{username}', exist_ok=True)
            filename = f'output/{username}/api_{timestamp}.json'
        
        # æ•´ç†è³‡æ–™æ ¼å¼
        output_data = {
            "user": user_data,
            "tweets": tweets,
            "metadata": {
                "total_tweets": len(tweets),
                "scraped_at": datetime.now().isoformat(),
                "method": "X API v2",
                "oldest_tweet": tweets[-1]['created_at'] if tweets else None,
                "newest_tweet": tweets[0]['created_at'] if tweets else None
            }
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å·²å„²å­˜åˆ°: {filename}")
        print(f"ğŸ“Š å…± {len(tweets)} å‰‡æ¨æ–‡")
        
        return filename


def main():
    """ä¸»ç¨‹å¼"""
    print("=" * 60)
    print("X API æ¨æ–‡çˆ¬èŸ²")
    print("=" * 60)
    print()
    
    # è¼¸å…¥ä½¿ç”¨è€…åç¨±
    username = input("è«‹è¼¸å…¥ X ä½¿ç”¨è€…åç¨± (ä¾‹å¦‚: elonmusk): ").strip()
    if not username:
        username = "elonmusk"  # é è¨­å€¼
    
    # ç§»é™¤ @ ç¬¦è™Ÿ
    username = username.replace('@', '')
    
    # è¼¸å…¥è¦æŠ“å–çš„æ¨æ–‡æ•¸é‡
    try:
        max_tweets = int(input("è¦æŠ“å–å¹¾å‰‡æ¨æ–‡? (å…è²»ç‰ˆå»ºè­° 100 ä»¥å…§): ") or "100")
    except ValueError:
        max_tweets = 100
    
    print()
    print("=" * 60)
    print()
    
    try:
        # åˆå§‹åŒ–çˆ¬èŸ²
        scraper = XAPIScraper()
        
        # å–å¾—ä½¿ç”¨è€… ID
        user_id, user_data = scraper.get_user_id(username)
        
        # æŠ“å–æ¨æ–‡
        tweets = scraper.get_user_tweets(user_id, max_results=max_tweets)
        
        if tweets:
            # å„²å­˜çµæœ
            scraper.save_tweets(tweets, user_data)
            
            print("\n" + "=" * 60)
            print("âœ… å®Œæˆ!")
            print("=" * 60)
        else:
            print("\nâš ï¸  æ²’æœ‰æŠ“åˆ°ä»»ä½•æ¨æ–‡")
    
    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
