import asyncio
import json
import os
from datetime import datetime
from typing import List, Dict, Optional
from playwright.async_api import async_playwright, Page, Browser
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()


class XScraper:
    """X (Twitter) çˆ¬èŸ²é¡åˆ¥ï¼Œä½¿ç”¨ Playwright é€²è¡Œç€è¦½å™¨è‡ªå‹•åŒ–"""
    
    def __init__(self, headless: bool = False):
        """
        åˆå§‹åŒ–çˆ¬èŸ²
        
        Args:
            headless: æ˜¯å¦ä½¿ç”¨ç„¡é ­æ¨¡å¼ï¼ˆä¸é¡¯ç¤ºç€è¦½å™¨ï¼‰
        """
        self.headless = headless
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.context = None
        self.cookies_file = 'x_cookies.json'
        
    async def start(self):
        """å•Ÿå‹•ç€è¦½å™¨"""
        self.playwright = await async_playwright().start()
        
        # ä½¿ç”¨æŒä¹…åŒ–ä¸Šä¸‹æ–‡ï¼Œä¿å­˜ç€è¦½å™¨è³‡æ–™ï¼ˆåŒ…å«ç™»å…¥ç‹€æ…‹ï¼‰
        user_data_dir = './browser_data'
        os.makedirs(user_data_dir, exist_ok=True)
        
        self.context = await self.playwright.chromium.launch_persistent_context(
            user_data_dir,
            headless=self.headless,
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            args=['--disable-blink-features=AutomationControlled']
        )
        
        print("âœ“ å·²å•Ÿå‹•ç€è¦½å™¨ï¼ˆä½¿ç”¨æŒä¹…åŒ–è³‡æ–™ï¼Œæœƒè¨˜ä½ç™»å…¥ç‹€æ…‹ï¼‰")
        
        self.page = self.context.pages[0] if self.context.pages else await self.context.new_page()
        self.browser = self.context
        
        # éš±è— webdriver ç‰¹å¾µ
        await self.page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)
        
    async def close(self):
        """é—œé–‰ç€è¦½å™¨"""
        if self.context:
            await self.context.close()
        if self.playwright:
            await self.playwright.stop()
    
    async def login(self, username: str = None, password: str = None):
        """
        ç™»å…¥ X å¹³å°ï¼ˆå¦‚æœéœ€è¦ï¼‰
        
        Args:
            username: X å¸³è™Ÿ
            password: X å¯†ç¢¼
        """
        username = username or os.getenv('X_USERNAME')
        password = password or os.getenv('X_PASSWORD')
        
        if not username or not password:
            print("æœªæä¾›ç™»å…¥è³‡è¨Šï¼Œå°‡ä»¥è¨ªå®¢æ¨¡å¼ç¹¼çºŒ...")
            return
        
        try:
            print("æ­£åœ¨å‰å¾€ç™»å…¥é é¢...")
            await self.page.goto('https://x.com/i/flow/login', wait_until='domcontentloaded', timeout=60000)
            await asyncio.sleep(3)
            
            # è¼¸å…¥ç”¨æˆ¶åæˆ–email
            print("è¼¸å…¥å¸³è™Ÿ...")
            username_input = self.page.locator('input[autocomplete="username"]')
            await username_input.wait_for(timeout=10000)
            await username_input.fill(username)
            await asyncio.sleep(1)
            
            # é»æ“Š Next æŒ‰éˆ•
            print("é»æ“Šä¸‹ä¸€æ­¥...")
            next_button = self.page.locator('button:has-text("Next"), button:has-text("ä¸‹ä¸€æ­¥")')
            await next_button.click()
            await asyncio.sleep(3)
            
            # è¼¸å…¥å¯†ç¢¼
            print("è¼¸å…¥å¯†ç¢¼...")
            password_input = self.page.locator('input[type="password"]')
            await password_input.wait_for(timeout=10000)
            await password_input.fill(password)
            await asyncio.sleep(1)
            
            # é»æ“Šç™»å…¥æŒ‰éˆ•
            print("é»æ“Šç™»å…¥...")
            login_button = self.page.locator('button:has-text("Log in"), button:has-text("ç™»å…¥")')
            await login_button.click()
            await asyncio.sleep(5)
            
            print("âœ“ ç™»å…¥æˆåŠŸ")
        except Exception as e:
            print(f"ç™»å…¥å¤±æ•—: {e}")
            print("å°‡ä»¥è¨ªå®¢æ¨¡å¼ç¹¼çºŒ...")
    
    async def scrape_user_profile(self, profile_url: str) -> Dict:
        """
        çˆ¬å–ç”¨æˆ¶åŸºæœ¬è³‡æ–™
        
        Args:
            profile_url: X ç”¨æˆ¶é é¢ç¶²å€
            
        Returns:
            åŒ…å«ç”¨æˆ¶è³‡æ–™çš„å­—å…¸
        """
        await self.page.goto(profile_url, wait_until='domcontentloaded', timeout=60000)
        await asyncio.sleep(5)
        
        # æ»¾å‹•é é¢ä»¥è¼‰å…¥å…§å®¹
        await self.page.evaluate('window.scrollTo(0, 400)')
        await asyncio.sleep(1)
        
        user_data = {}
        
        try:
            # æå–ç”¨æˆ¶åç¨±
            user_data['name'] = await self.page.locator('[data-testid="UserName"] span').first.text_content()
        except:
            user_data['name'] = 'Unknown'
        
        try:
            # æå–ç”¨æˆ¶ handle
            username_elem = await self.page.locator('[data-testid="UserName"]').text_content()
            if '@' in username_elem:
                user_data['username'] = username_elem.split('@')[1].split('\n')[0]
            else:
                user_data['username'] = profile_url.split('/')[-1]
        except:
            user_data['username'] = profile_url.split('/')[-1]
        
        try:
            # æå–ç°¡ä»‹
            user_data['bio'] = await self.page.locator('[data-testid="UserDescription"]').text_content()
        except:
            user_data['bio'] = ''
        
        try:
            # æå–é—œæ³¨æ•¸ã€ç²‰çµ²æ•¸ç­‰
            following_elem = await self.page.locator('a[href*="/following"] span').first.text_content()
            user_data['following'] = following_elem
        except:
            user_data['following'] = '0'
        
        try:
            followers_elem = await self.page.locator('a[href*="/verified_followers"] span').first.text_content()
            user_data['followers'] = followers_elem
        except:
            user_data['followers'] = '0'
        
        user_data['profile_url'] = profile_url
        user_data['scraped_at'] = datetime.now().isoformat()
        
        return user_data
    
    async def scrape_tweets(self, profile_url: str, max_tweets: int = 20) -> List[Dict]:
        """
        çˆ¬å–ç”¨æˆ¶æ¨æ–‡
        
        Args:
            profile_url: X ç”¨æˆ¶é é¢ç¶²å€
            max_tweets: æœ€å¤šçˆ¬å–çš„æ¨æ–‡æ•¸é‡
            
        Returns:
            æ¨æ–‡åˆ—è¡¨
        """
        await self.page.goto(profile_url, wait_until='domcontentloaded', timeout=60000)
        await asyncio.sleep(5)
        
        tweets = []
        seen_urls = set()
        scroll_count = 0
        max_scrolls = 500  # å¤§å¹…å¢åŠ æœ€å¤§æ»¾å‹•æ¬¡æ•¸
        no_new_tweets_count = 0
        last_tweet_count = 0
        
        print(f"é–‹å§‹çˆ¬å–æ¨æ–‡ï¼Œç›®æ¨™: {max_tweets} å‰‡...")
        print(f"å°‡æŒçºŒè‡ªå‹•æ»¾å‹•ç›´åˆ°é”åˆ°ç›®æ¨™æ•¸é‡...")
        
        # ç­‰å¾…æ¨æ–‡è¼‰å…¥
        try:
            await self.page.wait_for_selector('article[data-testid="tweet"]', timeout=10000)
        except:
            print("  âš ï¸ æœªæ‰¾åˆ°æ¨æ–‡å…ƒç´ ï¼Œå¯èƒ½éœ€è¦ç™»å…¥æˆ–é é¢çµæ§‹å·²è®Šæ›´")
            return tweets
        
        while len(tweets) < max_tweets and scroll_count < max_scrolls and no_new_tweets_count < 10:
            # æå–ç•¶å‰é é¢ä¸Šçš„æ‰€æœ‰æ¨æ–‡
            tweet_elements = await self.page.locator('article[data-testid="tweet"]').all()
            
            for tweet_elem in tweet_elements:
                if len(tweets) >= max_tweets:
                    break
                    
                try:
                    tweet_data = {}
                    
                    # æ¨æ–‡é€£çµï¼ˆç”¨æ–¼å»é‡ï¼‰
                    try:
                        links = await tweet_elem.locator('a[href*="/status/"]').all()
                        if links:
                            tweet_url = await links[0].get_attribute('href')
                            full_url = f"https://twitter.com{tweet_url}"
                            
                            # è·³éå·²è¦‹éçš„æ¨æ–‡
                            if full_url in seen_urls:
                                continue
                            
                            tweet_data['url'] = full_url
                            seen_urls.add(full_url)
                    except:
                        continue  # æ²’æœ‰ URL çš„è·³é
                    
                    # æ¨æ–‡æ–‡å­—å…§å®¹
                    try:
                        text_elem = tweet_elem.locator('[data-testid="tweetText"]')
                        tweet_data['text'] = await text_elem.text_content()
                    except:
                        tweet_data['text'] = ''
                    
                    # æ¨æ–‡æ™‚é–“
                    try:
                        time_elem = tweet_elem.locator('time')
                        tweet_data['timestamp'] = await time_elem.get_attribute('datetime')
                        display_time = await time_elem.text_content()
                        tweet_data['display_time'] = display_time
                    except:
                        tweet_data['timestamp'] = ''
                        tweet_data['display_time'] = ''
                    
                    # äº’å‹•æ•¸æ“šï¼šå›è¦†ã€è½‰æ¨ã€å–œæ­¡
                    try:
                        reply_elem = tweet_elem.locator('[data-testid="reply"]')
                        tweet_data['replies'] = await reply_elem.get_attribute('aria-label') or '0'
                    except:
                        tweet_data['replies'] = '0'
                    
                    try:
                        retweet_elem = tweet_elem.locator('[data-testid="retweet"]')
                        tweet_data['retweets'] = await retweet_elem.get_attribute('aria-label') or '0'
                    except:
                        tweet_data['retweets'] = '0'
                    
                    try:
                        like_elem = tweet_elem.locator('[data-testid="like"]')
                        tweet_data['likes'] = await like_elem.get_attribute('aria-label') or '0'
                    except:
                        tweet_data['likes'] = '0'
                    
                    # åªæ·»åŠ æœ‰æ–‡å­—å…§å®¹çš„æ¨æ–‡
                    if tweet_data.get('text') or tweet_data.get('url'):
                        tweets.append(tweet_data)
                        if len(tweets) % 10 == 0:  # æ¯ 10 å‰‡é¡¯ç¤ºä¸€æ¬¡
                            print(f"  âœ“ å·²çˆ¬å– {len(tweets)}/{max_tweets} å‰‡")
                        
                except Exception as e:
                    continue
            
            # æª¢æŸ¥æœ¬è¼ªæ˜¯å¦æœ‰æ–°æ¨æ–‡
            if len(tweets) == last_tweet_count:
                no_new_tweets_count += 1
                print(f"  âš ï¸  ç¬¬ {scroll_count + 1} æ¬¡æ»¾å‹•ï¼Œç„¡æ–°æ¨æ–‡ ({no_new_tweets_count}/10)")
            else:
                no_new_tweets_count = 0
                if len(tweets) % 10 != 0:  # ä¸æ˜¯æ•´åæ•¸æ™‚ä¹Ÿé¡¯ç¤º
                    print(f"  ğŸ“Š ç¬¬ {scroll_count + 1} æ¬¡æ»¾å‹•ï¼Œç›®å‰å…± {len(tweets)} å‰‡")
            
            last_tweet_count = len(tweets)
            
            # ä½¿ç”¨å¤šç¨®æ»¾å‹•æ–¹å¼ç¢ºä¿é é¢æ»¾å‹•
            if len(tweets) < max_tweets:
                print(f"  ğŸ”„ è‡ªå‹•æ»¾å‹•ç¬¬ {scroll_count + 1} æ¬¡...")
                # æ–¹æ³•1: æ»¾å‹•åˆ°é é¢åº•éƒ¨
                await self.page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                await asyncio.sleep(1)
                
                # æ–¹æ³•2: å†æ»¾å‹•ä¸€æ¬¡ç¢ºä¿è§¸ç™¼è¼‰å…¥
                await self.page.evaluate('window.scrollBy(0, 1000)')
                await asyncio.sleep(1)
            
            scroll_count += 1
        
        if no_new_tweets_count >= 10:
            print(f"  å·²é€£çºŒ 10 è¼ªæ²’æœ‰æ–°æ¨æ–‡ï¼Œåœæ­¢æ»¾å‹•")
        
        print(f"âœ“ å®Œæˆï¼å…±çˆ¬å– {len(tweets)} å‰‡æ¨æ–‡")
        return tweets[:max_tweets]
    
    async def scrape_full_profile(self, profile_url: str, max_tweets: int = 20) -> Dict:
        """
        çˆ¬å–å®Œæ•´çš„ç”¨æˆ¶è³‡æ–™ï¼ˆåŒ…å«å€‹äººè³‡æ–™å’Œæ¨æ–‡ï¼‰
        
        Args:
            profile_url: X ç”¨æˆ¶é é¢ç¶²å€
            max_tweets: æœ€å¤šçˆ¬å–çš„æ¨æ–‡æ•¸é‡
            
        Returns:
            å®Œæ•´çš„ç”¨æˆ¶è³‡æ–™å­—å…¸
        """
        print(f"\né–‹å§‹çˆ¬å–: {profile_url}")
        print("=" * 60)
        
        # çˆ¬å–ç”¨æˆ¶è³‡æ–™
        print("\n[1/2] çˆ¬å–ç”¨æˆ¶åŸºæœ¬è³‡æ–™...")
        user_data = await self.scrape_user_profile(profile_url)
        
        # çˆ¬å–æ¨æ–‡
        print("\n[2/2] çˆ¬å–æ¨æ–‡...")
        tweets = await self.scrape_tweets(profile_url, max_tweets)
        
        # çµ„åˆçµæœ
        result = {
            'user': user_data,
            'tweets': tweets,
            'total_tweets_scraped': len(tweets)
        }
        
        return result


def save_to_json(data: Dict, username: str = None, filename: str = None):
    """
    å°‡è³‡æ–™å„²å­˜ç‚º JSON æª”æ¡ˆï¼ŒæŒ‰ç”¨æˆ¶åå»ºç«‹è³‡æ–™å¤¾
    
    Args:
        data: è¦å„²å­˜çš„è³‡æ–™
        username: ç”¨æˆ¶åï¼ˆç”¨æ–¼å»ºç«‹è³‡æ–™å¤¾ï¼‰
        filename: æª”æ¡ˆåç¨±ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œæœƒè‡ªå‹•ç”Ÿæˆï¼‰
    """
    # å¦‚æœ username æ˜¯ Noneï¼Œå¾ data ä¸­å–å¾—
    if not username:
        username = data.get('user', {}).get('username', 'unknown')
    
    if not filename:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"tweets_{timestamp}.json"
    
    # å»ºç«‹ç”¨æˆ¶å°ˆå±¬è³‡æ–™å¤¾
    user_folder = os.path.join('output', username)
    os.makedirs(user_folder, exist_ok=True)
    filepath = os.path.join(user_folder, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ“ è³‡æ–™å·²å„²å­˜è‡³: {filepath}")
    return filepath


async def main():
    """ä¸»ç¨‹å¼"""
    print("\n" + "=" * 60)
    print("X (Twitter) çˆ¬èŸ²ç³»çµ±")
    print("=" * 60)
    
    # å–å¾—ç”¨æˆ¶è¼¸å…¥
    profile_url = input("\nè«‹è¼¸å…¥ X ç”¨æˆ¶é é¢ç¶²å€ (ä¾‹å¦‚: https://twitter.com/username): ").strip()
    
    if not profile_url:
        print("éŒ¯èª¤: è«‹æä¾›æœ‰æ•ˆçš„ç¶²å€")
        return
    
    # æ¨™æº–åŒ–ç¶²å€æ ¼å¼
    if not profile_url.startswith('http'):
        profile_url = f"https://twitter.com/{profile_url}"
    
    # è©¢å•æ˜¯å¦éœ€è¦ç™»å…¥
    need_login = input("\næ˜¯å¦éœ€è¦ç™»å…¥? (å¦‚æœç›®æ¨™å¸³è™Ÿæœ‰éš±ç§è¨­ç½®) [y/N]: ").strip().lower()
    
    # è©¢å•è¦çˆ¬å–çš„æ¨æ–‡æ•¸é‡
    try:
        max_tweets = int(input("\nè¦çˆ¬å–å¤šå°‘å‰‡æ¨æ–‡? [é è¨­: 20]: ").strip() or "20")
    except:
        max_tweets = 20
    
    # è©¢å•æ˜¯å¦é¡¯ç¤ºç€è¦½å™¨
    show_browser = input("\næ˜¯å¦é¡¯ç¤ºç€è¦½å™¨è¦–çª—? [Y/n]: ").strip().lower()
    headless = show_browser == 'n'
    
    # é–‹å§‹çˆ¬å–
    scraper = XScraper(headless=headless)
    
    try:
        await scraper.start()
        
        # å¦‚æœéœ€è¦ç™»å…¥
        if need_login == 'y':
            username = input("X å¸³è™Ÿ: ").strip()
            password = input("X å¯†ç¢¼: ").strip()
            await scraper.login(username, password)
        
        # çˆ¬å–è³‡æ–™
        data = await scraper.scrape_full_profile(profile_url, max_tweets)
        
        # å„²å­˜çµæœ
        save_to_json(data)
        
        print("\n" + "=" * 60)
        print("çˆ¬å–å®Œæˆï¼")
        print("=" * 60)
        print(f"\nç”¨æˆ¶åç¨±: {data['user']['name']}")
        print(f"ç”¨æˆ¶ ID: @{data['user']['username']}")
        print(f"é—œæ³¨æ•¸: {data['user']['following']}")
        print(f"ç²‰çµ²æ•¸: {data['user']['followers']}")
        print(f"çˆ¬å–æ¨æ–‡æ•¸: {data['total_tweets_scraped']}")
        
    except Exception as e:
        print(f"\néŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        await scraper.close()


if __name__ == '__main__':
    asyncio.run(main())
