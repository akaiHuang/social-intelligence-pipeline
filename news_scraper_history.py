"""
åŠ å¯†è²¨å¹£æ–°èçˆ¬èŸ² - æ­·å²æ–‡ç« ç‰ˆ
é€éæœå°‹å’Œåˆ†é¡é é¢çˆ¬å–æ­·å²æ–‡ç« 
"""
import asyncio
import json
import os
from datetime import datetime
from playwright.async_api import async_playwright
import re

class HistoryNewsScraper:
    def __init__(self):
        self.keywords = [
            'Bitcoin', 'æ¯”ç‰¹å¹£',
            'BTC',
        ]
        
        self.output_dir = 'output/news_history'
        os.makedirs(self.output_dir, exist_ok=True)
    
    def matches_keywords(self, text):
        """æª¢æŸ¥æ–‡å­—æ˜¯å¦åŒ…å«é—œéµå­—"""
        if not text:
            return False
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in self.keywords)
    
    async def scrape_blocktempo_category(self, page, max_pages=10):
        """
        çˆ¬å–å‹•å€å‹•è¶¨çš„åˆ†é¡é é¢
        å¯ä»¥ç¿»é æŠ“å–æ›´å¤šæ­·å²æ–‡ç« 
        """
        print("\nğŸ” æ­£åœ¨çˆ¬å–ï¼šå‹•å€å‹•è¶¨ - åŠ å¯†è²¨å¹£å¸‚å ´åˆ†é¡")
        print("=" * 60)
        
        base_url = "https://www.blocktempo.com/category/cryptocurrency-market"
        articles = []
        
        for page_num in range(1, max_pages + 1):
            try:
                # æ§‹å»ºåˆ†é  URL
                if page_num == 1:
                    url = base_url
                else:
                    url = f"{base_url}/page/{page_num}"
                
                print(f"\nğŸ“„ ç¬¬ {page_num} é : {url}")
                
                await page.goto(url, wait_until='domcontentloaded', timeout=60000)
                await page.wait_for_timeout(2000)
                
                # æª¢æŸ¥æ˜¯å¦é‚„æœ‰å…§å®¹
                article_elements = await page.query_selector_all('article, .post-item, .article-item')
                
                if not article_elements:
                    print(f"âš ï¸  ç¬¬ {page_num} é æ²’æœ‰æ‰¾åˆ°æ–‡ç« ,åœæ­¢çˆ¬å–")
                    break
                
                print(f"æ‰¾åˆ° {len(article_elements)} ç¯‡æ–‡ç« ")
                
                for elem in article_elements:
                    try:
                        # æå–æ¨™é¡Œ
                        title_elem = await elem.query_selector('h2, h3, .title, .post-title, a')
                        title = await title_elem.inner_text() if title_elem else ''
                        
                        # æå–é€£çµ
                        link_elem = await elem.query_selector('a')
                        link = await link_elem.get_attribute('href') if link_elem else ''
                        if link and not link.startswith('http'):
                            link = f"https://www.blocktempo.com{link}"
                        
                        # æå–æ‘˜è¦
                        summary_elem = await elem.query_selector('.excerpt, .summary, p')
                        summary = await summary_elem.inner_text() if summary_elem else ''
                        
                        # æå–æ—¥æœŸ - å„ªå…ˆå¾ URL æå–
                        year = None
                        date = ''
                        
                        # æ–¹æ³•1: å¾ URL æå– (æœ€å¯é )
                        if link:
                            url_match = re.search(r'/(\d{4})/(\d{2})/(\d{2})/', link)
                            if url_match:
                                year = int(url_match.group(1))
                                month = url_match.group(2)
                                day = url_match.group(3)
                                date = f"{year}-{month}-{day}"
                        
                        # æ–¹æ³•2: å¾æ—¥æœŸå…ƒç´ æå–
                        if not year:
                            date_elem = await elem.query_selector('time, .date, .post-date, [datetime]')
                            if date_elem:
                                date = await date_elem.inner_text()
                                # å˜—è©¦å¾ datetime å±¬æ€§å–å¾—
                                if not date:
                                    date = await date_elem.get_attribute('datetime')
                                
                                if date:
                                    year_match = re.search(r'20\d{2}', date)
                                    if year_match:
                                        year = int(year_match.group())
                        
                        if title and link:
                            articles.append({
                                'title': title.strip(),
                                'link': link,
                                'summary': summary.strip()[:200],
                                'date': date.strip(),
                                'year': year,
                                'source': 'BlockTempo',
                                'scraped_at': datetime.now().isoformat()
                            })
                            
                            # é¡¯ç¤ºå¹´ä»½è³‡è¨Š
                            year_info = f"[{year}]" if year else "[å¹´ä»½æœªçŸ¥]"
                            print(f"  âœ“ {year_info} {title[:40]}...")
                    
                    except Exception as e:
                        continue
                
                await page.wait_for_timeout(1000)  # é¿å…è«‹æ±‚éå¿«
                
            except Exception as e:
                print(f"âŒ ç¬¬ {page_num} é çˆ¬å–å¤±æ•—: {e}")
                break
        
        print(f"\nâœ… BlockTempo: å…±æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        return articles
    
    async def scrape_abmedia_category(self, page, max_pages=10):
        """
        çˆ¬å–éˆæ–°èçš„æ¯”ç‰¹å¹£åˆ†é¡
        """
        print("\nğŸ” æ­£åœ¨çˆ¬å–ï¼šéˆæ–°è - æ¯”ç‰¹å¹£åˆ†é¡")
        print("=" * 60)
        
        base_url = "https://abmedia.io/category/invsetments/bitcoin"
        articles = []
        
        for page_num in range(1, max_pages + 1):
            try:
                # æ§‹å»ºåˆ†é  URL
                if page_num == 1:
                    url = base_url
                else:
                    url = f"{base_url}/page/{page_num}"
                
                print(f"\nğŸ“„ ç¬¬ {page_num} é : {url}")
                
                await page.goto(url, wait_until='domcontentloaded', timeout=60000)
                await page.wait_for_timeout(2000)
                
                # æª¢æŸ¥æ˜¯å¦é‚„æœ‰å…§å®¹
                article_elements = await page.query_selector_all('article, .post, .news-item')
                
                if not article_elements:
                    print(f"âš ï¸  ç¬¬ {page_num} é æ²’æœ‰æ‰¾åˆ°æ–‡ç« ,åœæ­¢çˆ¬å–")
                    break
                
                print(f"æ‰¾åˆ° {len(article_elements)} ç¯‡æ–‡ç« ")
                
                for elem in article_elements:
                    try:
                        # æå–æ¨™é¡Œ
                        title_elem = await elem.query_selector('h1, h2, h3, .title, a')
                        title = await title_elem.inner_text() if title_elem else ''
                        
                        # æå–é€£çµ
                        link_elem = await elem.query_selector('a')
                        link = await link_elem.get_attribute('href') if link_elem else ''
                        if link and not link.startswith('http'):
                            link = f"https://abmedia.io{link}"
                        
                        # æå–æ‘˜è¦
                        summary_elem = await elem.query_selector('.excerpt, .description, p')
                        summary = await summary_elem.inner_text() if summary_elem else ''
                        
                        # æå–æ—¥æœŸ - å„ªå…ˆå¾ URL æå–
                        year = None
                        date = ''
                        
                        # æ–¹æ³•1: å¾ URL æå– (æœ€å¯é )
                        if link:
                            url_match = re.search(r'/(\d{4})/(\d{2})/(\d{2})/', link)
                            if url_match:
                                year = int(url_match.group(1))
                                month = url_match.group(2)
                                day = url_match.group(3)
                                date = f"{year}-{month}-{day}"
                        
                        # æ–¹æ³•2: å¾æ—¥æœŸå…ƒç´ æå–
                        if not year:
                            date_elem = await elem.query_selector('time, .date, .meta, [datetime]')
                            if date_elem:
                                date = await date_elem.inner_text()
                                if not date:
                                    date = await date_elem.get_attribute('datetime')
                                
                                if date:
                                    year_match = re.search(r'20\d{2}', date)
                                    if year_match:
                                        year = int(year_match.group())
                        
                        if title and link:
                            articles.append({
                                'title': title.strip(),
                                'link': link,
                                'summary': summary.strip()[:200],
                                'date': date.strip(),
                                'year': year,
                                'source': 'ABMedia',
                                'scraped_at': datetime.now().isoformat()
                            })
                            
                            year_info = f"[{year}]" if year else "[å¹´ä»½æœªçŸ¥]"
                            print(f"  âœ“ {year_info} {title[:40]}...")
                    
                    except Exception as e:
                        continue
                
                await page.wait_for_timeout(1000)
                
            except Exception as e:
                print(f"âŒ ç¬¬ {page_num} é çˆ¬å–å¤±æ•—: {e}")
                break
        
        print(f"\nâœ… ABMedia: å…±æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        return articles
    
    async def scrape_blockcast_category(self, page, max_pages=10):
        """
        çˆ¬å–å€å¡Šå®¢çš„æ¯”ç‰¹å¹£ç›¸é—œæ–‡ç« 
        """
        print("\nğŸ” æ­£åœ¨çˆ¬å–ï¼šå€å¡Šå®¢ - å¸‚å ´å¹£åƒ¹åˆ†é¡")
        print("=" * 60)
        
        base_url = "https://blockcast.it/category/news/market/price"
        articles = []
        
        for page_num in range(1, max_pages + 1):
            try:
                # æ§‹å»ºåˆ†é  URL
                if page_num == 1:
                    url = base_url
                else:
                    url = f"{base_url}/page/{page_num}"
                
                print(f"\nğŸ“„ ç¬¬ {page_num} é : {url}")
                
                await page.goto(url, wait_until='domcontentloaded', timeout=60000)
                await page.wait_for_timeout(2000)
                
                # æª¢æŸ¥æ˜¯å¦é‚„æœ‰å…§å®¹
                article_elements = await page.query_selector_all('article, .post-item, .news-item')
                
                if not article_elements:
                    print(f"âš ï¸  ç¬¬ {page_num} é æ²’æœ‰æ‰¾åˆ°æ–‡ç« ,åœæ­¢çˆ¬å–")
                    break
                
                print(f"æ‰¾åˆ° {len(article_elements)} ç¯‡æ–‡ç« ")
                
                for elem in article_elements:
                    try:
                        # æå–æ¨™é¡Œ
                        title_elem = await elem.query_selector('h1, h2, h3, .title, a')
                        title = await title_elem.inner_text() if title_elem else ''
                        
                        # æå–é€£çµ
                        link_elem = await elem.query_selector('a')
                        link = await link_elem.get_attribute('href') if link_elem else ''
                        if link and not link.startswith('http'):
                            link = f"https://blockcast.it{link}"
                        
                        # æå–æ‘˜è¦
                        summary_elem = await elem.query_selector('.excerpt, p')
                        summary = await summary_elem.inner_text() if summary_elem else ''
                        
                        # æå–æ—¥æœŸ
                        date_elem = await elem.query_selector('time, .date')
                        date = ''
                        if date_elem:
                            date = await date_elem.inner_text()
                            if not date:
                                date = await date_elem.get_attribute('datetime')
                        
                        # æå–å¹´ä»½
                        year = None
                        if date:
                            year_match = re.search(r'20\d{2}', date)
                            if year_match:
                                year = int(year_match.group())
                        
                        # åªä¿ç•™æ¯”ç‰¹å¹£ç›¸é—œæ–‡ç« 
                        if title and link and self.matches_keywords(f"{title} {summary}"):
                            articles.append({
                                'title': title.strip(),
                                'link': link,
                                'summary': summary.strip()[:200],
                                'date': date.strip(),
                                'year': year,
                                'source': 'Blockcast',
                                'scraped_at': datetime.now().isoformat()
                            })
                            
                            year_info = f"[{year}]" if year else "[å¹´ä»½æœªçŸ¥]"
                            print(f"  âœ“ {year_info} {title[:40]}...")
                    
                    except Exception as e:
                        continue
                
                await page.wait_for_timeout(1000)
                
            except Exception as e:
                print(f"âŒ ç¬¬ {page_num} é çˆ¬å–å¤±æ•—: {e}")
                break
        
        print(f"\nâœ… Blockcast: å…±æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        return articles
    
    def save_by_year(self, all_articles):
        """æŒ‰å¹´ä»½å„²å­˜æ–‡ç« """
        if not all_articles:
            print("\nâš ï¸  æ²’æœ‰æ‰¾åˆ°æ–‡ç« ")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # æŒ‰å¹´ä»½åˆ†é¡
        by_year = {}
        no_year = []
        
        for article in all_articles:
            year = article.get('year')
            if year:
                if year not in by_year:
                    by_year[year] = []
                by_year[year].append(article)
            else:
                no_year.append(article)
        
        # å„²å­˜å„å¹´ä»½
        for year in sorted(by_year.keys()):
            articles = by_year[year]
            year_dir = os.path.join(self.output_dir, str(year))
            os.makedirs(year_dir, exist_ok=True)
            
            filename = os.path.join(year_dir, f'bitcoin_news_{timestamp}.json')
            
            output_data = {
                'year': year,
                'total_articles': len(articles),
                'articles': articles,
                'by_source': {},
                'scraped_at': datetime.now().isoformat()
            }
            
            # çµ±è¨ˆä¾†æº
            for article in articles:
                source = article['source']
                output_data['by_source'][source] = output_data['by_source'].get(source, 0) + 1
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ {year} å¹´: å·²å„²å­˜ {len(articles)} ç¯‡æ–‡ç« åˆ° {filename}")
            print(f"   ä¾†æºåˆ†å¸ƒ: {output_data['by_source']}")
        
        # å„²å­˜å¹´ä»½æœªçŸ¥çš„æ–‡ç« 
        if no_year:
            unknown_dir = os.path.join(self.output_dir, 'unknown_year')
            os.makedirs(unknown_dir, exist_ok=True)
            filename = os.path.join(unknown_dir, f'bitcoin_news_{timestamp}.json')
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump({
                    'year': None,
                    'total_articles': len(no_year),
                    'articles': no_year,
                    'scraped_at': datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ å¹´ä»½æœªçŸ¥: å·²å„²å­˜ {len(no_year)} ç¯‡æ–‡ç« ")
        
        # ç”Ÿæˆç¸½è¦½
        summary_file = os.path.join(self.output_dir, f'summary_{timestamp}.json')
        summary = {
            'total_articles': len(all_articles),
            'by_year': {str(year): len(articles) for year, articles in by_year.items()},
            'unknown_year': len(no_year),
            'year_range': {
                'earliest': min(by_year.keys()) if by_year else None,
                'latest': max(by_year.keys()) if by_year else None
            },
            'by_source': {},
            'scraped_at': datetime.now().isoformat()
        }
        
        for article in all_articles:
            source = article['source']
            summary['by_source'][source] = summary['by_source'].get(source, 0) + 1
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print("\n" + "=" * 60)
        print("ğŸ“Š ç¸½è¦½çµ±è¨ˆ")
        print("=" * 60)
        print(f"ç¸½å…±: {len(all_articles)} ç¯‡æ–‡ç« ")
        print(f"å¹´ä»½ç¯„åœ: {summary['year_range']['earliest']} - {summary['year_range']['latest']}")
        print(f"å„å¹´ä»½æ–‡ç« æ•¸: {summary['by_year']}")
        print(f"ä¾†æºçµ±è¨ˆ: {summary['by_source']}")
        print(f"ç¸½è¦½æª”æ¡ˆ: {summary_file}")
    
    async def scrape_all(self, max_pages_per_site=10):
        """çˆ¬å–æ‰€æœ‰ç¶²ç«™çš„æ­·å²æ–‡ç« """
        print("=" * 60)
        print("ğŸš€ é–‹å§‹çˆ¬å–æ¯”ç‰¹å¹£æ­·å²æ–°è")
        print("=" * 60)
        print(f"ğŸ”‘ é—œéµå­—: {', '.join(self.keywords)}")
        print(f"ğŸ“„ æ¯å€‹ç¶²ç«™æœ€å¤šçˆ¬å– {max_pages_per_site} é ")
        print("=" * 60)
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            all_articles = []
            
            try:
                # çˆ¬å–å„ç¶²ç«™
                articles = await self.scrape_blocktempo_category(page, max_pages_per_site)
                all_articles.extend(articles)
                
                articles = await self.scrape_abmedia_category(page, max_pages_per_site)
                all_articles.extend(articles)
                
                articles = await self.scrape_blockcast_category(page, max_pages_per_site)
                all_articles.extend(articles)
                
            except Exception as e:
                print(f"\nâŒ çˆ¬å–éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            
            finally:
                await browser.close()
            
            # æŒ‰å¹´ä»½å„²å­˜
            self.save_by_year(all_articles)
            
            print("\n" + "=" * 60)
            print("âœ… çˆ¬å–å®Œæˆ!")
            print("=" * 60)


async def main():
    print("æ¯”ç‰¹å¹£æ­·å²æ–°èçˆ¬èŸ²")
    print()
    
    # è¼¸å…¥è¦çˆ¬å–çš„é æ•¸
    try:
        max_pages = int(input("æ¯å€‹ç¶²ç«™è¦çˆ¬å–å¹¾é ? (å»ºè­° 5-20 é ï¼Œé è¨­ 10): ") or "10")
    except ValueError:
        max_pages = 10
    
    print()
    scraper = HistoryNewsScraper()
    await scraper.scrape_all(max_pages_per_site=max_pages)


if __name__ == "__main__":
    asyncio.run(main())
