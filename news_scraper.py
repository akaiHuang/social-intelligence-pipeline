"""
åŠ å¯†è²¨å¹£æ–°èçˆ¬èŸ²
æ”¯æ´å¤šå€‹å°ç£ä¸»æµå€å¡Šéˆæ–°èç¶²ç«™
"""
import asyncio
import json
import os
from datetime import datetime
from playwright.async_api import async_playwright
import re

class NewsScraper:
    def __init__(self):
        self.keywords = [
            'BTC', 'Bitcoin', 'æ¯”ç‰¹å¹£', 'æ¯”ç‰¹å¸',
            'Elon Musk', 'Elon', 'Musk', 'é¦¬æ–¯å…‹', 'é©¬æ–¯å…‹',
            'Trump', 'å·æ™®', 'ç‰¹æœ—æ™®',
            'Michael Saylor', 'Saylor',
            'CZ', 'è¶™é•·éµ¬',
        ]
        
        self.output_dir = 'output/news'
        os.makedirs(self.output_dir, exist_ok=True)
    
    def matches_keywords(self, text):
        """æª¢æŸ¥æ–‡å­—æ˜¯å¦åŒ…å«é—œéµå­—"""
        if not text:
            return False
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in self.keywords)
    
    async def scrape_blocktempo(self, page, max_articles=50):
        """
        çˆ¬å–å‹•å€å‹•è¶¨ BlockTempo
        URL: https://www.blocktempo.com
        """
        print("\nğŸ” æ­£åœ¨çˆ¬å–ï¼šå‹•å€å‹•è¶¨ (BlockTempo)")
        print("=" * 60)
        
        url = "https://www.blocktempo.com"
        await page.goto(url, wait_until='domcontentloaded', timeout=60000)
        await page.wait_for_timeout(3000)
        
        articles = []
        
        # çˆ¬å–æ–‡ç« åˆ—è¡¨
        article_elements = await page.query_selector_all('article, .post-item, .article-item')
        
        print(f"ğŸ“„ æ‰¾åˆ° {len(article_elements)} å€‹æ–‡ç« å…ƒç´ ")
        
        for elem in article_elements[:max_articles]:
            try:
                # æå–æ¨™é¡Œ
                title_elem = await elem.query_selector('h2, h3, .title, .post-title')
                title = await title_elem.inner_text() if title_elem else ''
                
                # æå–é€£çµ
                link_elem = await elem.query_selector('a')
                link = await link_elem.get_attribute('href') if link_elem else ''
                if link and not link.startswith('http'):
                    link = f"{url}{link}"
                
                # æå–æ‘˜è¦
                summary_elem = await elem.query_selector('.excerpt, .summary, p')
                summary = await summary_elem.inner_text() if summary_elem else ''
                
                # æå–æ—¥æœŸ
                date_elem = await elem.query_selector('time, .date, .post-date')
                date = await date_elem.inner_text() if date_elem else ''
                
                # æª¢æŸ¥æ˜¯å¦åŒ¹é…é—œéµå­—
                if title and (self.matches_keywords(title) or self.matches_keywords(summary)):
                    articles.append({
                        'title': title.strip(),
                        'link': link,
                        'summary': summary.strip()[:200],
                        'date': date.strip(),
                        'source': 'BlockTempo',
                        'scraped_at': datetime.now().isoformat()
                    })
                    print(f"  âœ“ {title[:50]}...")
                
            except Exception as e:
                continue
        
        print(f"\nâœ… BlockTempo: æ‰¾åˆ° {len(articles)} ç¯‡ç›¸é—œæ–‡ç« ")
        return articles
    
    async def scrape_abmedia(self, page, max_articles=50):
        """
        çˆ¬å–éˆæ–°è ABMedia
        URL: https://abmedia.io
        """
        print("\nğŸ” æ­£åœ¨çˆ¬å–ï¼šéˆæ–°è (ABMedia)")
        print("=" * 60)
        
        url = "https://abmedia.io"
        await page.goto(url, wait_until='domcontentloaded', timeout=60000)
        await page.wait_for_timeout(3000)
        
        articles = []
        
        # çˆ¬å–æ–‡ç« åˆ—è¡¨
        article_elements = await page.query_selector_all('article, .post, .news-item')
        
        print(f"ğŸ“„ æ‰¾åˆ° {len(article_elements)} å€‹æ–‡ç« å…ƒç´ ")
        
        for elem in article_elements[:max_articles]:
            try:
                # æå–æ¨™é¡Œ
                title_elem = await elem.query_selector('h1, h2, h3, .title')
                title = await title_elem.inner_text() if title_elem else ''
                
                # æå–é€£çµ
                link_elem = await elem.query_selector('a')
                link = await link_elem.get_attribute('href') if link_elem else ''
                if link and not link.startswith('http'):
                    link = f"{url}{link}"
                
                # æå–æ‘˜è¦
                summary_elem = await elem.query_selector('.excerpt, .description, p')
                summary = await summary_elem.inner_text() if summary_elem else ''
                
                # æå–æ—¥æœŸ
                date_elem = await elem.query_selector('time, .date')
                date = await date_elem.inner_text() if date_elem else ''
                
                # æª¢æŸ¥æ˜¯å¦åŒ¹é…é—œéµå­—
                if title and (self.matches_keywords(title) or self.matches_keywords(summary)):
                    articles.append({
                        'title': title.strip(),
                        'link': link,
                        'summary': summary.strip()[:200],
                        'date': date.strip(),
                        'source': 'ABMedia',
                        'scraped_at': datetime.now().isoformat()
                    })
                    print(f"  âœ“ {title[:50]}...")
                
            except Exception as e:
                continue
        
        print(f"\nâœ… ABMedia: æ‰¾åˆ° {len(articles)} ç¯‡ç›¸é—œæ–‡ç« ")
        return articles
    
    async def scrape_blockcast(self, page, max_articles=50):
        """
        çˆ¬å–å€å¡Šå®¢ Blockcast
        URL: https://blockcast.it
        """
        print("\nğŸ” æ­£åœ¨çˆ¬å–ï¼šå€å¡Šå®¢ (Blockcast)")
        print("=" * 60)
        
        url = "https://blockcast.it"
        await page.goto(url, wait_until='domcontentloaded', timeout=60000)
        await page.wait_for_timeout(3000)
        
        articles = []
        
        # çˆ¬å–æ–‡ç« åˆ—è¡¨
        article_elements = await page.query_selector_all('article, .post-item')
        
        print(f"ğŸ“„ æ‰¾åˆ° {len(article_elements)} å€‹æ–‡ç« å…ƒç´ ")
        
        for elem in article_elements[:max_articles]:
            try:
                # æå–æ¨™é¡Œ
                title_elem = await elem.query_selector('h1, h2, h3, .title')
                title = await title_elem.inner_text() if title_elem else ''
                
                # æå–é€£çµ
                link_elem = await elem.query_selector('a')
                link = await link_elem.get_attribute('href') if link_elem else ''
                if link and not link.startswith('http'):
                    link = f"{url}{link}"
                
                # æå–æ‘˜è¦
                summary_elem = await elem.query_selector('.excerpt, p')
                summary = await summary_elem.inner_text() if summary_elem else ''
                
                # æå–æ—¥æœŸ
                date_elem = await elem.query_selector('time, .date')
                date = await date_elem.inner_text() if date_elem else ''
                
                # æª¢æŸ¥æ˜¯å¦åŒ¹é…é—œéµå­—
                if title and (self.matches_keywords(title) or self.matches_keywords(summary)):
                    articles.append({
                        'title': title.strip(),
                        'link': link,
                        'summary': summary.strip()[:200],
                        'date': date.strip(),
                        'source': 'Blockcast',
                        'scraped_at': datetime.now().isoformat()
                    })
                    print(f"  âœ“ {title[:50]}...")
                
            except Exception as e:
                continue
        
        print(f"\nâœ… Blockcast: æ‰¾åˆ° {len(articles)} ç¯‡ç›¸é—œæ–‡ç« ")
        return articles
    
    async def scrape_cnyes(self, page, max_articles=50):
        """
        çˆ¬å–é‰…äº¨ç¶²å€å¡Šéˆ
        URL: https://news.cnyes.com/news/cat/bc
        """
        print("\nğŸ” æ­£åœ¨çˆ¬å–ï¼šé‰…äº¨ç¶²å€å¡Šéˆ (Cnyes)")
        print("=" * 60)
        
        url = "https://news.cnyes.com/news/cat/bc"
        await page.goto(url, wait_until='domcontentloaded', timeout=60000)
        await page.wait_for_timeout(3000)
        
        articles = []
        
        # çˆ¬å–æ–‡ç« åˆ—è¡¨
        article_elements = await page.query_selector_all('article, .news-item, ._1m83')
        
        print(f"ğŸ“„ æ‰¾åˆ° {len(article_elements)} å€‹æ–‡ç« å…ƒç´ ")
        
        for elem in article_elements[:max_articles]:
            try:
                # æå–æ¨™é¡Œ
                title_elem = await elem.query_selector('h3, h2, a')
                title = await title_elem.inner_text() if title_elem else ''
                
                # æå–é€£çµ
                link_elem = await elem.query_selector('a')
                link = await link_elem.get_attribute('href') if link_elem else ''
                if link and not link.startswith('http'):
                    link = f"https://news.cnyes.com{link}"
                
                # æå–æ‘˜è¦
                summary_elem = await elem.query_selector('p, .summary')
                summary = await summary_elem.inner_text() if summary_elem else ''
                
                # æå–æ—¥æœŸ
                date_elem = await elem.query_selector('time, .date, ._2jPO')
                date = await date_elem.inner_text() if date_elem else ''
                
                # æª¢æŸ¥æ˜¯å¦åŒ¹é…é—œéµå­—
                if title and (self.matches_keywords(title) or self.matches_keywords(summary)):
                    articles.append({
                        'title': title.strip(),
                        'link': link,
                        'summary': summary.strip()[:200],
                        'date': date.strip(),
                        'source': 'Cnyes',
                        'scraped_at': datetime.now().isoformat()
                    })
                    print(f"  âœ“ {title[:50]}...")
                
            except Exception as e:
                continue
        
        print(f"\nâœ… Cnyes: æ‰¾åˆ° {len(articles)} ç¯‡ç›¸é—œæ–‡ç« ")
        return articles
    
    def categorize_article(self, article):
        """æ ¹æ“šå…§å®¹åˆ†é¡æ–‡ç« """
        title = article['title'].lower()
        summary = article['summary'].lower()
        content = f"{title} {summary}"
        
        # BTC ç›¸é—œ
        if any(k in content for k in ['btc', 'bitcoin', 'æ¯”ç‰¹å¹£', 'æ¯”ç‰¹å¸']):
            return 'bitcoin'
        
        # Elon Musk ç›¸é—œ
        if any(k in content for k in ['elon', 'musk', 'é¦¬æ–¯å…‹', 'é©¬æ–¯å…‹']):
            return 'elon_musk'
        
        # Trump ç›¸é—œ
        if any(k in content for k in ['trump', 'å·æ™®', 'ç‰¹æœ—æ™®']):
            return 'trump'
        
        # Michael Saylor ç›¸é—œ
        if any(k in content for k in ['saylor', 'microstrategy']):
            return 'saylor'
        
        # CZ ç›¸é—œ
        if any(k in content for k in ['cz', 'è¶™é•·éµ¬', 'binance', 'å¹£å®‰']):
            return 'cz'
        
        return 'other'
    
    def save_articles(self, all_articles):
        """å„²å­˜æ–‡ç« ä¸¦æŒ‰é¡åˆ¥åˆ†é¡"""
        if not all_articles:
            print("\nâš ï¸  æ²’æœ‰æ‰¾åˆ°ç›¸é—œæ–‡ç« ")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # æŒ‰é¡åˆ¥åˆ†é¡
        categorized = {}
        for article in all_articles:
            category = self.categorize_article(article)
            if category not in categorized:
                categorized[category] = []
            categorized[category].append(article)
        
        # å„²å­˜å„é¡åˆ¥
        for category, articles in categorized.items():
            category_dir = os.path.join(self.output_dir, category)
            os.makedirs(category_dir, exist_ok=True)
            
            filename = os.path.join(category_dir, f'news_{timestamp}.json')
            
            output_data = {
                'category': category,
                'total_articles': len(articles),
                'articles': articles,
                'keywords': self.keywords,
                'scraped_at': datetime.now().isoformat()
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ {category}: å·²å„²å­˜ {len(articles)} ç¯‡æ–‡ç« åˆ° {filename}")
        
        # å„²å­˜ç¸½è¦½
        summary_file = os.path.join(self.output_dir, f'summary_{timestamp}.json')
        summary = {
            'total_articles': len(all_articles),
            'by_category': {cat: len(arts) for cat, arts in categorized.items()},
            'by_source': {},
            'scraped_at': datetime.now().isoformat()
        }
        
        # çµ±è¨ˆä¾†æº
        for article in all_articles:
            source = article['source']
            summary['by_source'][source] = summary['by_source'].get(source, 0) + 1
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“Š ç¸½è¦½: {summary_file}")
        print(f"ç¸½å…± {len(all_articles)} ç¯‡æ–‡ç« ")
        print(f"åˆ†é¡çµ±è¨ˆ: {summary['by_category']}")
        print(f"ä¾†æºçµ±è¨ˆ: {summary['by_source']}")
    
    async def scrape_all(self):
        """çˆ¬å–æ‰€æœ‰ç¶²ç«™"""
        print("=" * 60)
        print("ğŸš€ é–‹å§‹çˆ¬å–åŠ å¯†è²¨å¹£æ–°è")
        print("=" * 60)
        print(f"ğŸ”‘ é—œéµå­—: {', '.join(self.keywords)}")
        print("=" * 60)
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            all_articles = []
            
            try:
                # çˆ¬å–å„ç¶²ç«™
                articles = await self.scrape_blocktempo(page)
                all_articles.extend(articles)
                
                articles = await self.scrape_abmedia(page)
                all_articles.extend(articles)
                
                articles = await self.scrape_blockcast(page)
                all_articles.extend(articles)
                
                articles = await self.scrape_cnyes(page)
                all_articles.extend(articles)
                
            except Exception as e:
                print(f"\nâŒ çˆ¬å–éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            
            finally:
                await browser.close()
            
            # å„²å­˜çµæœ
            self.save_articles(all_articles)
            
            print("\n" + "=" * 60)
            print("âœ… çˆ¬å–å®Œæˆ!")
            print("=" * 60)


async def main():
    scraper = NewsScraper()
    await scraper.scrape_all()


if __name__ == "__main__":
    asyncio.run(main())
